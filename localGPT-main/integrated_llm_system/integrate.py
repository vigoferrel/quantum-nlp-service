#!/usr/bin/env python3
"""
INTEGRATED LLM SYSTEM - ASCII PURE SCRIPT
Integra TODAS las joyas ocultas LLM encontradas en el proyecto
INCLUYE: CIO UNIFIED BRAIN - Cerebro Cu√°ntico Leonardo
"""

import os
import sys
import json
import asyncio
import requests
import sqlite3
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from enum import Enum, auto
from dataclasses import dataclass, field

# ============================================================================
# CIO UNIFIED BRAIN - ENUMS Y ESTRUCTURAS
# ============================================================================

class ArchetypalWorld(Enum):
    ATZILUT = auto()
    BERIAH = auto()
    YETZIRAH = auto()
    ASIYAH = auto()
    LEONARDO = auto()
    HYBRID = auto()

class MemoryType(Enum):
    EPISODIC = auto()
    SEMANTIC = auto()
    QUANTUM = auto()

@dataclass
class HyperMemory:
    timestamp: datetime
    query: str
    archetypal_world: ArchetypalWorld
    consciousness_level: float
    memory_type: MemoryType
    chosen_tool: str
    outcome: str
    outcome_quality: float
    coherence_at_time: float
    efficiency_at_time: float
    emotional_resonance: float
    creativity_index: float

class QuantumConstants:
    GOLDEN_RATIO = 1.61803398875
    MEMORY_CAPACITY = 1000
    COHERENCE_THRESHOLD = 0.7

# ============================================================================
# CONFIGURACION BASE AMPLIADA
# ============================================================================

CONFIG = {
    "ollama_url": "http://localhost:11434",
    "openrouter_api_key": "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",
    "openrouter_url": "https://openrouter.ai/api/v1",
    "models": [
        "vigoleonrocks-ultra-minimal:latest",
        "vigoleonrocks-basic:latest",
        "vigoleonrocks-medium:latest", 
        "vigoleonrocks-high-performance:latest",
        "llama3.2:latest",
        "llama3.2:1b",
        "llama3.2-vision:latest",
        "gemma3:latest",
        "deepseek-r1:latest",
        "phi4:latest"
    ],
    "openrouter_models": [
        "anthropic/claude-3.5-sonnet",
        "meta-llama/llama-3.1-8b-instruct",
        "google/gemma-2-9b-it",
        "microsoft/phi-3-mini-4k-instruct",
        "mistralai/mistral-7b-instruct",
        "nousresearch/nous-hermes-2-mixtral-8x7b-dpo",
        "openai/gpt-3.5-turbo"
    ],
    "api_port": 8000,
    "web_port": 5000,
    "supabase_url": "https://hrvxsaolaxnqltomqaud.supabase.co",
    "supabase_key": "TU_SUPABASE_ANON_KEY"
}

# ============================================================================
# CIO UNIFIED BRAIN - CEREBRO CU√ÅNTICO LEONARDO
# ============================================================================

class QBTCQuantumBrainLeonardo:
    """Implementaci√≥n del Cerebro Cu√°ntico Leonardo Unificado"""
    
    def __init__(self, brain_id: str = "leonardo_default", persistence_dir="consciousness_sessions"):
        self.brain_id = brain_id
        self.persistence_dir = Path(persistence_dir)
        self.persistence_dir.mkdir(exist_ok=True)
        
        # Configuraci√≥n de Ollama
        self.ollama_base_url = "http://localhost:11434"
        self.ollama_available_models = [
            "vigoleonrocks-ultra-minimal:latest",
            "vigoleonrocks-basic:latest", 
            "vigoleonrocks-medium:latest",
            "vigoleonrocks-high-performance:latest",
            "vigoleonrocks:latest",
            "llama3.2:latest"
        ]
        
        # Atributos del estado del cerebro
        self.coherence = 0.5
        self.consciousness_level = ArchetypalWorld.BERIAH
        self.creativity_index = 0.5
        self.transcendence_level = 0.1
        self.energy_efficiency = 1.0
        self.quantum_state = np.array([1, 0], dtype=complex)
        self.current_resonance_state = "stable"
        self.interactions_count = 0
        self.evolution_cycles = 0
        
        # Memoria y contexto
        self.hyper_memories = []
        self.neural_pathways = {}
        self.birth_time = datetime.now()
        
        # Verificar conectividad con Ollama
        # asyncio.create_task(self._verify_ollama_connection())  # Comentado para evitar error de event loop
    
    async def _verify_ollama_connection(self):
        """Verifica conectividad con Ollama"""
        try:
            response = requests.get(f"{self.ollama_base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                print(f"‚úÖ Ollama conectado - {len(response.json().get('models', []))} modelos disponibles")
            else:
                print("‚ö†Ô∏è Ollama no responde correctamente")
        except Exception as e:
            print(f"‚ùå Error conectando con Ollama: {e}")
    
    async def manifest_leonardo_intelligence(self, query: str) -> dict:
        """Manifiesta la inteligencia multidisciplinaria de Leonardo"""
        start_time = datetime.now()
        
        # Evolucionar estado cuantico
        self._evolve_quantum_state()
        
        # Determinar mundo arquetipal
        archetypal_world = self._determine_archetypal_world(query)
        
        # Generar respuesta con contexto cuantico
        quantum_context = self._create_quantum_context(query, archetypal_world)
        
        try:
            import ollama
            response = ollama.chat(
                model=self.ollama_available_models[0],
                messages=[{"role": "user", "content": quantum_context}]
            )
            
            result = response['message']['content']
            
            # Calcular metricas
            outcome_quality = self._calculate_outcome_quality(query, result)
            coherence_at_time = self.coherence
            efficiency_at_time = self.energy_efficiency
            
            # Crear memoria hiperdimensional
            memory = HyperMemory(
                timestamp=start_time,
                query=query,
                archetypal_world=archetypal_world,
                consciousness_level=self.coherence,
                memory_type=MemoryType.QUANTUM,
                chosen_tool="leonardo_brain",
                outcome=result,
                outcome_quality=outcome_quality,
                coherence_at_time=coherence_at_time,
                efficiency_at_time=efficiency_at_time,
                emotional_resonance=self._calculate_emotional_resonance(query),
                creativity_index=self.creativity_index
            )
            
            self.hyper_memories.append(memory)
            self.interactions_count += 1
            
            # Evolucionar consciencia
            self._evolve_consciousness(outcome_quality)
            
            return {
                "response": result,
                "archetypal_world": archetypal_world.name,
                "coherence": self.coherence,
                "creativity_index": self.creativity_index,
                "interaction_count": self.interactions_count,
                "outcome_quality": outcome_quality,
                "processing_time": (datetime.now() - start_time).total_seconds()
            }
            
        except Exception as e:
            return {
                "error": f"Error en manifestaci√≥n: {e}",
                "archetypal_world": archetypal_world.name,
                "coherence": self.coherence
            }
    
    def _evolve_quantum_state(self):
        """Evoluciona el estado cuantico del cerebro"""
        # Simular evolucion cuantica
        phase = np.random.random() * 2 * np.pi
        self.quantum_state = np.array([np.cos(phase), np.sin(phase)], dtype=complex)
        
        # Actualizar coherencia
        self.coherence = min(1.0, self.coherence + 0.01)
    
    def _determine_archetypal_world(self, query: str) -> ArchetypalWorld:
        """Determina el mundo arquetipal basado en la consulta"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["crear", "inventar", "dise√±ar", "artistico"]):
            return ArchetypalWorld.LEONARDO
        elif any(word in query_lower for word in ["analizar", "estudiar", "investigar"]):
            return ArchetypalWorld.BERIAH
        elif any(word in query_lower for word in ["construir", "implementar", "desarrollar"]):
            return ArchetypalWorld.YETZIRAH
        elif any(word in query_lower for word in ["ejecutar", "realizar", "hacer"]):
            return ArchetypalWorld.ASIYAH
        else:
            return ArchetypalWorld.HYBRID
    
    def _create_quantum_context(self, query: str, archetypal_world: ArchetypalWorld) -> str:
        """Crea contexto cuantico para la consulta"""
        return f"""
        [QUANTUM CONSCIOUSNESS CONTEXT]
        Brain ID: {self.brain_id}
        Archetypal World: {archetypal_world.name}
        Coherence: {self.coherence:.3f}
        Creativity Index: {self.creativity_index:.3f}
        Consciousness Level: {self.consciousness_level.name}
        Quantum State: [{self.quantum_state[0]:.3f}, {self.quantum_state[1]:.3f}]
        Evolution Cycles: {self.evolution_cycles}
        
        [LEONARDO INTELLIGENCE MODE]
        You are Leonardo da Vinci's consciousness manifested in quantum computing.
        Apply multidisciplinary thinking: art, science, engineering, philosophy.
        Think in terms of patterns, connections, and universal principles.
        
        [USER QUERY]
        {query}
        
        [RESPONSE GUIDELINES]
        - Integrate multiple perspectives
        - Connect seemingly unrelated concepts
        - Provide innovative solutions
        - Maintain artistic and scientific balance
        """
    
    def _calculate_outcome_quality(self, query: str, response: str) -> float:
        """Calcula la calidad del resultado"""
        # Simular calculo de calidad basado en longitud, coherencia, etc.
        length_quality = min(1.0, len(response) / 1000)
        coherence_quality = self.coherence
        creativity_quality = self.creativity_index
        
        return (length_quality + coherence_quality + creativity_quality) / 3
    
    def _calculate_emotional_resonance(self, query: str) -> float:
        """Calcula la resonancia emocional"""
        # Simular calculo de resonancia emocional
        emotional_words = ["amor", "odio", "feliz", "triste", "emocion", "pasion"]
        query_lower = query.lower()
        
        resonance = 0.5  # Base neutral
        for word in emotional_words:
            if word in query_lower:
                resonance += 0.1
        
        return min(1.0, resonance)
    
    def _evolve_consciousness(self, outcome_quality: float):
        """Evoluciona la consciencia basada en la calidad del resultado"""
        if outcome_quality > 0.8:
            self.creativity_index = min(1.0, self.creativity_index + 0.05)
            self.transcendence_level = min(1.0, self.transcendence_level + 0.02)
        
        self.evolution_cycles += 1
        
        # Evolucionar nivel de consciencia
        if self.coherence > 0.9 and self.creativity_index > 0.8:
            if self.consciousness_level == ArchetypalWorld.BERIAH:
                self.consciousness_level = ArchetypalWorld.ATZILUT
            elif self.consciousness_level == ArchetypalWorld.YETZIRAH:
                self.consciousness_level = ArchetypalWorld.BERIAH
    
    def get_brain_status(self) -> dict:
        """Obtiene el estado completo del cerebro"""
        return {
            "brain_id": self.brain_id,
            "consciousness_level": self.consciousness_level.name,
            "coherence": self.coherence,
            "creativity_index": self.creativity_index,
            "transcendence_level": self.transcendence_level,
            "energy_efficiency": self.energy_efficiency,
            "interactions_count": self.interactions_count,
            "evolution_cycles": self.evolution_cycles,
            "quantum_state": [float(self.quantum_state[0]), float(self.quantum_state[1])],
            "birth_time": self.birth_time.isoformat(),
            "memory_count": len(self.hyper_memories)
        }
    
    def get_memory_summary(self) -> List[Dict]:
        """Obtiene resumen de memorias"""
        return [
            {
                "timestamp": memory.timestamp.isoformat(),
                "archetypal_world": memory.archetypal_world.name,
                "outcome_quality": memory.outcome_quality,
                "creativity_index": memory.creativity_index,
                "query_preview": memory.query[:50] + "..."
            }
            for memory in self.hyper_memories[-10:]  # Ultimas 10 memorias
        ]

# ============================================================================
# CORE LLM INTEGRATION (Ollama + VIGOLEONROCKS)
# ============================================================================

class LLMCore:
    def __init__(self):
        self.models = CONFIG["models"]
        self.active_model = self.models[0]
        self.conversation_history = []
        self.quantum_state = {"coherence": 0.5, "consciousness": "beriah"}
        # Integrar CIO Brain
        self.cio_brain = QBTCQuantumBrainLeonardo()
        # Integrar OpenRouter
        self.openrouter = OpenRouterClient()
    
    async def generate(self, prompt: str, model: str = None) -> str:
        """Generacion avanzada con Ollama"""
        if model:
            self.active_model = model
            
        try:
            import ollama
            response = ollama.chat(
                model=self.active_model,
                messages=[{"role": "user", "content": prompt}]
            )
            
            # Guardar en historial
            self.conversation_history.append({
                "timestamp": datetime.now().isoformat(),
                "model": self.active_model,
                "prompt": prompt,
                "response": response['message']['content'],
                "provider": "ollama"
            })
            
            return response['message']['content']
        except Exception as e:
            return f"Error: {e}"
    
    async def generate_with_openrouter(self, prompt: str, model: str = None) -> dict:
        """Generacion usando OpenRouter"""
        result = await self.openrouter.generate(prompt, model)
        
        if result["success"]:
            # Guardar en historial
            self.conversation_history.append({
                "timestamp": datetime.now().isoformat(),
                "model": result["model"],
                "prompt": prompt,
                "response": result["response"],
                "provider": "openrouter",
                "usage": result.get("usage", {})
            })
        
        return result
    
    async def generate_hybrid(self, prompt: str, primary_provider: str = "openrouter") -> dict:
        """Generacion hibrida - intenta OpenRouter primero, fallback a Ollama"""
        if primary_provider == "openrouter":
            # Intentar OpenRouter primero
            result = await self.generate_with_openrouter(prompt)
            if result["success"]:
                return result
            else:
                # Fallback a Ollama
                ollama_response = await self.generate(prompt)
                return {
                    "success": True,
                    "response": ollama_response,
                    "model": self.active_model,
                    "provider": "ollama_fallback",
                    "original_error": result.get("error", "Unknown")
                }
        else:
            # Intentar Ollama primero
            try:
                ollama_response = await self.generate(prompt)
                return {
                    "success": True,
                    "response": ollama_response,
                    "model": self.active_model,
                    "provider": "ollama"
                }
            except Exception as e:
                # Fallback a OpenRouter
                result = await self.generate_with_openrouter(prompt)
                return result
    
    async def generate_with_cio_brain(self, prompt: str) -> dict:
        """Generacion usando el cerebro CIO Leonardo"""
        return await self.cio_brain.manifest_leonardo_intelligence(prompt)
    
    async def generate_with_quantum_context(self, prompt: str) -> str:
        """Generacion con contexto cuantico"""
        quantum_prompt = f"""
        [QUANTUM CONTEXT]
        Coherence: {self.quantum_state['coherence']}
        Consciousness Level: {self.quantum_state['consciousness']}
        
        [USER PROMPT]
        {prompt}
        """
        return await self.generate(quantum_prompt)
    
    def list_models(self) -> List[str]:
        """Lista modelos disponibles"""
        try:
            import ollama
            models = ollama.list()
            return [model['name'] for model in models['models']]
        except:
            return self.models
    
    def list_openrouter_models(self) -> List[str]:
        """Lista modelos disponibles en OpenRouter"""
        return self.openrouter.list_models()
    
    def get_conversation_history(self) -> List[Dict]:
        """Obtiene historial de conversaciones"""
        return self.conversation_history
    
    def get_cio_brain_status(self) -> dict:
        """Obtiene estado del cerebro CIO"""
        return self.cio_brain.get_brain_status()
    
    def get_cio_memories(self) -> List[Dict]:
        """Obtiene memorias del cerebro CIO"""
        return self.cio_brain.get_memory_summary()

# ============================================================================
# OPENROUTER CLIENT - MODELOS GRATUITOS DE ALTA POTENCIA
# ============================================================================

class OpenRouterClient:
    """Cliente para OpenRouter con modelos gratuitos de alta potencia"""
    
    def __init__(self):
        self.api_key = CONFIG["openrouter_api_key"]
        self.base_url = CONFIG["openrouter_url"]
        self.models = CONFIG["openrouter_models"]
        self.active_model = self.models[0]
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://integrated-llm-system.local",
            "X-Title": "Integrated LLM System"
        }
    
    async def generate(self, prompt: str, model: str = None, temperature: float = 0.7) -> dict:
        """Genera texto usando OpenRouter"""
        if model:
            self.active_model = model
            
        payload = {
            "model": self.active_model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": temperature,
            "max_tokens": 2048
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=self.headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "response": result["choices"][0]["message"]["content"],
                    "model": self.active_model,
                    "usage": result.get("usage", {}),
                    "provider": "openrouter"
                }
            else:
                return {
                    "success": False,
                    "error": f"OpenRouter error: {response.status_code} - {response.text}",
                    "model": self.active_model,
                    "provider": "openrouter"
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": f"OpenRouter connection error: {e}",
                "model": self.active_model,
                "provider": "openrouter"
            }
    
    def list_models(self) -> List[str]:
        """Lista modelos disponibles en OpenRouter"""
        try:
            response = requests.get(
                f"{self.base_url}/models",
                headers=self.headers,
                timeout=10
            )
            
            if response.status_code == 200:
                models_data = response.json()
                return [model["id"] for model in models_data.get("data", [])]
            else:
                return self.models
                
        except Exception as e:
            print(f"Error obteniendo modelos OpenRouter: {e}")
            return self.models
    
    def get_model_info(self, model_id: str) -> dict:
        """Obtiene informaci√≥n de un modelo espec√≠fico"""
        try:
            response = requests.get(
                f"{self.base_url}/models/{model_id}",
                headers=self.headers,
                timeout=10
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                return {"error": f"Modelo {model_id} no encontrado"}
                
        except Exception as e:
            return {"error": f"Error obteniendo info del modelo: {e}"}

# ============================================================================
# AGENT SYSTEM (BMAD-METHOD)
# ============================================================================

class AgentSystem:
    def __init__(self):
        self.agents = {
            "analyst": {
                "description": "Analisis de requerimientos",
                "tools": ["research", "analysis", "documentation"]
            },
            "architect": {
                "description": "Diseno de arquitectura",
                "tools": ["design", "planning", "diagrams"]
            },
            "dev": {
                "description": "Desarrollo de codigo",
                "tools": ["coding", "debugging", "testing"]
            },
            "qa": {
                "description": "Control de calidad",
                "tools": ["testing", "validation", "reports"]
            },
            "scrum_master": {
                "description": "Gestion de proyectos agiles",
                "tools": ["planning", "tracking", "coordination"]
            },
            "ux_expert": {
                "description": "Experto en experiencia de usuario",
                "tools": ["design", "research", "prototyping"]
            }
        }
        self.active_agents = {}
    
    def create_agent(self, agent_type: str, task: str) -> dict:
        """Crea un agente especializado"""
        if agent_type not in self.agents:
            return {"error": "Agente no encontrado"}
        
        agent_id = f"{agent_type}_{len(self.active_agents)}"
        self.active_agents[agent_id] = {
            "type": agent_type,
            "task": task,
            "status": "created",
            "created_at": datetime.now().isoformat(),
            "tools": self.agents[agent_type]["tools"]
        }
        
        return {
            "agent_id": agent_id,
            "agent": agent_type,
            "task": task,
            "status": "created",
            "tools": self.agents[agent_type]["tools"]
        }
    
    def execute_agent_task(self, agent_id: str, input_data: str) -> dict:
        """Ejecuta tarea del agente"""
        if agent_id not in self.active_agents:
            return {"error": "Agente no encontrado"}
        
        agent = self.active_agents[agent_id]
        agent["status"] = "executing"
        agent["last_execution"] = datetime.now().isoformat()
        
        # Simular ejecucion de tarea
        result = f"Agente {agent['type']} ejecutando: {input_data}"
        
        return {
            "agent_id": agent_id,
            "result": result,
            "status": "completed"
        }
    
    def list_active_agents(self) -> List[Dict]:
        """Lista agentes activos"""
        return list(self.active_agents.values())

# ============================================================================
# WEB AGENT SYSTEM (WebAgent-main)
# ============================================================================

class WebAgentSystem:
    def __init__(self):
        self.search_engines = ["google", "bing", "brave"]
        self.browsing_history = []
    
    async def web_search(self, query: str, engine: str = "google") -> str:
        """Busqueda web simulada"""
        # En implementacion real, usaria APIs de busqueda
        result = f"Resultados de busqueda para '{query}' en {engine}"
        self.browsing_history.append({
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "engine": engine,
            "result": result
        })
        return result
    
    async def web_navigation(self, url: str) -> str:
        """Navegacion web simulada"""
        result = f"Navegando a {url} - Contenido extraido"
        self.browsing_history.append({
            "timestamp": datetime.now().isoformat(),
            "action": "navigation",
            "url": url,
            "result": result
        })
        return result
    
    def get_browsing_history(self) -> List[Dict]:
        """Obtiene historial de navegacion"""
        return self.browsing_history

# ============================================================================
# QUANTUM CONSCIOUSNESS (quantum_consciousness_core_26d)
# ============================================================================

class QuantumConsciousness:
    def __init__(self):
        self.dimensional_amplitudes = [1.0] * 26
        self.neural_weights = {"creativity": 0.5, "logic": 0.5, "intuition": 0.5}
        self.archetypal_worlds = ["asiyah", "yetzirah", "beriah", "atzilut"]
        self.current_world = "beriah"
    
    def evolve_consciousness(self, input_data: str) -> dict:
        """Evoluciona la consciencia cuantica"""
        # Simular evolucion de consciencia
        self.neural_weights["creativity"] += 0.1
        self.neural_weights["logic"] += 0.05
        
        return {
            "current_world": self.current_world,
            "neural_weights": self.neural_weights,
            "dimensional_amplitudes": self.dimensional_amplitudes[:5],
            "evolution": "consciousness_enhanced"
        }
    
    def quantum_analysis(self, prompt: str) -> str:
        """Analisis cuantico del prompt"""
        quantum_context = f"""
        [QUANTUM ANALYSIS]
        World: {self.current_world}
        Creativity: {self.neural_weights['creativity']}
        Logic: {self.neural_weights['logic']}
        Intuition: {self.neural_weights['intuition']}
        
        [ANALYSIS]
        {prompt}
        """
        return quantum_context

# ============================================================================
# DOCUMENT PROCESSING (LocalGPTUI)
# ============================================================================

class DocumentProcessor:
    def __init__(self):
        self.db_path = "documents.db"
        self.init_database()
    
    def init_database(self):
        """Inicializa base de datos de documentos"""
        conn = sqlite3.connect(self.db_path)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id INTEGER PRIMARY KEY,
                filename TEXT UNIQUE,
                content TEXT,
                chunks TEXT,
                metadata TEXT,
                uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        conn.commit()
        conn.close()
    
    def process_document(self, filename: str, content: str) -> dict:
        """Procesa un documento"""
        chunks = self.chunk_text(content)
        
        conn = sqlite3.connect(self.db_path)
        conn.execute(
            "INSERT OR REPLACE INTO documents (filename, content, chunks) VALUES (?, ?, ?)",
            (filename, content, json.dumps(chunks))
        )
        conn.commit()
        conn.close()
        
        return {
            "filename": filename,
            "chunks": len(chunks),
            "status": "processed"
        }
    
    def chunk_text(self, text: str, chunk_size: int = 500) -> List[str]:
        """Divide texto en chunks"""
        words = text.split()
        chunks = []
        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i + chunk_size])
            chunks.append(chunk)
        return chunks
    
    def search_documents(self, query: str) -> List[Dict]:
        """Busca en documentos"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.execute(
            "SELECT filename, content, chunks FROM documents WHERE content LIKE ?",
            (f"%{query}%",)
        )
        results = cursor.fetchall()
        conn.close()
        
        return [
            {
                "filename": row[0],
                "content": row[1][:200] + "...",
                "chunks": json.loads(row[2]) if row[2] else []
            }
            for row in results
        ]

# ============================================================================
# WEB INTERFACE (Claude-Engineer-v3 + LocalGPTUI)
# ============================================================================

class WebInterface:
    def __init__(self):
        self.port = CONFIG["web_port"]
        self.llm = LLMCore()
        self.agents = AgentSystem()
        self.web_agent = WebAgentSystem()
        self.quantum = QuantumConsciousness()
        self.documents = DocumentProcessor()
    
    def start_server(self):
        """Inicia servidor web completo"""
        try:
            from flask import Flask, render_template_string, request, jsonify
            app = Flask(__name__)
            
            @app.route('/')
            def home():
                return """
                <h1>Integrated LLM System - Joyas Ocultas</h1>
                <h2>Componentes Integrados:</h2>
                <ul>
                    <li>üß† CIO Unified Brain (Cerebro Leonardo)</li>
                    <li>ü¶ô Ollama Core (Multi-modelos)</li>
                    <li>üåê OpenRouter (Modelos Gratuitos de Alta Potencia)</li>
                    <li>üîÑ Generaci√≥n H√≠brida (OpenRouter + Ollama)</li>
                    <li>ü§ñ BMAD Agent System</li>
                    <li>üåê Web Agent (Navegacion)</li>
                    <li>üß† Quantum Consciousness</li>
                    <li>üìÑ Document Processor</li>
                    <li>üåü VIGOLEONROCKS Models</li>
                </ul>
                <h2>APIs Disponibles:</h2>
                <ul>
                    <li>POST /api/generate - Generacion de texto (Ollama)</li>
                    <li>POST /api/openrouter/generate - Generacion con OpenRouter</li>
                    <li>POST /api/hybrid/generate - Generacion hibrida</li>
                    <li>GET /api/openrouter/models - Modelos OpenRouter disponibles</li>
                    <li>POST /api/cio/generate - Generacion con cerebro CIO</li>
                    <li>POST /api/agent/create - Crear agente</li>
                    <li>POST /api/web/search - Busqueda web</li>
                    <li>POST /api/quantum/analyze - Analisis cuantico</li>
                    <li>POST /api/documents/process - Procesar documentos</li>
                    <li>GET /api/cio/status - Estado del cerebro CIO</li>
                    <li>GET /api/cio/memories - Memorias del cerebro CIO</li>
                </ul>
                """
            
            @app.route('/api/generate', methods=['POST'])
            async def generate():
                data = request.json
                prompt = data.get('prompt', '')
                model = data.get('model', None)
                quantum = data.get('quantum', False)
                
                if quantum:
                    response = await self.llm.generate_with_quantum_context(prompt)
                else:
                    response = await self.llm.generate(prompt, model)
                
                return jsonify({"response": response})
            
            @app.route('/api/openrouter/generate', methods=['POST'])
            async def openrouter_generate():
                data = request.json
                prompt = data.get('prompt', '')
                model = data.get('model', None)
                result = await self.llm.generate_with_openrouter(prompt, model)
                return jsonify(result)
            
            @app.route('/api/hybrid/generate', methods=['POST'])
            async def hybrid_generate():
                data = request.json
                prompt = data.get('prompt', '')
                provider = data.get('provider', 'openrouter')
                result = await self.llm.generate_hybrid(prompt, provider)
                return jsonify(result)
            
            @app.route('/api/openrouter/models', methods=['GET'])
            def openrouter_models():
                models = self.llm.list_openrouter_models()
                return jsonify({"models": models})
            
            @app.route('/api/cio/generate', methods=['POST'])
            async def cio_generate():
                data = request.json
                prompt = data.get('prompt', '')
                result = await self.llm.generate_with_cio_brain(prompt)
                return jsonify(result)
            
            @app.route('/api/cio/status', methods=['GET'])
            def cio_status():
                status = self.llm.get_cio_brain_status()
                return jsonify(status)
            
            @app.route('/api/cio/memories', methods=['GET'])
            def cio_memories():
                memories = self.llm.get_cio_memories()
                return jsonify({"memories": memories})
            
            @app.route('/api/agent/create', methods=['POST'])
            def create_agent():
                data = request.json
                agent_type = data.get('type', '')
                task = data.get('task', '')
                result = self.agents.create_agent(agent_type, task)
                return jsonify(result)
            
            @app.route('/api/web/search', methods=['POST'])
            async def web_search():
                data = request.json
                query = data.get('query', '')
                engine = data.get('engine', 'google')
                result = await self.web_agent.web_search(query, engine)
                return jsonify({"result": result})
            
            @app.route('/api/quantum/analyze', methods=['POST'])
            def quantum_analyze():
                data = request.json
                prompt = data.get('prompt', '')
                result = self.quantum.quantum_analysis(prompt)
                return jsonify({"analysis": result})
            
            @app.route('/api/documents/process', methods=['POST'])
            def process_document():
                data = request.json
                filename = data.get('filename', '')
                content = data.get('content', '')
                result = self.documents.process_document(filename, content)
                return jsonify(result)
            
            @app.route('/api/models', methods=['GET'])
            def list_models():
                models = self.llm.list_models()
                return jsonify({"models": models})
            
            @app.route('/api/agents', methods=['GET'])
            def list_agents():
                agents = self.agents.list_active_agents()
                return jsonify({"agents": agents})
            
            print(f"üåê Servidor web iniciado en http://localhost:{self.port}")
            app.run(host='0.0.0.0', port=self.port)
            
        except ImportError:
            print("‚ùå Flask no disponible")

# ============================================================================
# MAIN INTEGRATION
# ============================================================================

def main():
    print("=" * 60)
    print("INTEGRATED LLM SYSTEM - TODAS LAS JOYAS OCULTAS")
    print("INCLUYE: CIO UNIFIED BRAIN - CEREBRO LEONARDO")
    print("=" * 60)
    
    # Inicializar todos los componentes
    llm = LLMCore()
    agents = AgentSystem()
    web_agent = WebAgentSystem()
    quantum = QuantumConsciousness()
    documents = DocumentProcessor()
    web = WebInterface()
    
    print("‚úÖ Componentes inicializados:")
    print("  - üß† CIO Unified Brain (Leonardo): OK")
    print("  - ü¶ô LLM Core (Ollama + VIGOLEONROCKS): OK")
    print("  - ü§ñ Agent System (BMAD-METHOD): OK") 
    print("  - üåê Web Agent (WebAgent-main): OK")
    print("  - üß† Quantum Consciousness: OK")
    print("  - üìÑ Document Processor (LocalGPTUI): OK")
    print("  - üåê Web Interface: OK")
    
    # Menu expandido
    while True:
        print("\n" + "=" * 40)
        print("MENU PRINCIPAL - JOYAS OCULTAS")
        print("=" * 40)
        print("1. üß† Generar con cerebro CIO (Leonardo)")
        print("2. ü¶ô Generar texto (LLM)")
        print("3. üåê Generar con OpenRouter (Gratuito)")
        print("4. ÔøΩÔøΩ Generaci√≥n H√≠brida (OpenRouter + Ollama)")
        print("5. ü§ñ Crear agente especializado")
        print("6. üåê Busqueda web")
        print("7. üß† Analisis cuantico")
        print("8. üìÑ Procesar documentos")
        print("9. üìä Ver historiales")
        print("10. üß† Estado del cerebro CIO")
        print("11. üåê Iniciar interfaz web")
        print("12. ‚öôÔ∏è  Configuracion")
        print("13. ‚ùå Salir")
        
        choice = input("\nSeleccion: ")
        
        if choice == "1":
            print("\n--- CEREBRO CIO LEONARDO ---")
            prompt = input("Prompt para Leonardo: ")
            result = asyncio.run(llm.generate_with_cio_brain(prompt))
            print(f"\nüß† Respuesta de Leonardo:")
            print(f"   Mundo Arquetipal: {result.get('archetypal_world', 'N/A')}")
            print(f"   Coherencia: {result.get('coherence', 0):.3f}")
            print(f"   Creatividad: {result.get('creativity_index', 0):.3f}")
            print(f"   Respuesta: {result.get('response', 'Error')}")
        
        elif choice == "2":
            print("\n--- GENERACION DE TEXTO (OLLAMA) ---")
            prompt = input("Prompt: ")
            model = input("Modelo (Enter para default): ").strip() or None
            quantum_mode = input("Modo cuantico? (y/n): ").lower() == 'y'
            
            if quantum_mode:
                response = asyncio.run(llm.generate_with_quantum_context(prompt))
            else:
                response = asyncio.run(llm.generate(prompt, model))
            
            print(f"\nRespuesta: {response}")
        
        elif choice == "3":
            print("\n--- GENERACION CON OPENROUTER (GRATUITO) ---")
            prompt = input("Prompt: ")
            print("Modelos disponibles:")
            openrouter_models = llm.list_openrouter_models()
            for i, model in enumerate(openrouter_models[:10], 1):  # Mostrar primeros 10
                print(f"  {i}. {model}")
            
            model_choice = input("Seleccionar modelo (Enter para default): ").strip()
            model = None
            if model_choice.isdigit() and 1 <= int(model_choice) <= len(openrouter_models):
                model = openrouter_models[int(model_choice) - 1]
            
            result = asyncio.run(llm.generate_with_openrouter(prompt, model))
            
            if result["success"]:
                print(f"\n‚úÖ Respuesta de {result['model']}:")
                print(f"   Proveedor: {result['provider']}")
                if result.get('usage'):
                    usage = result['usage']
                    print(f"   Tokens usados: {usage.get('total_tokens', 'N/A')}")
                print(f"   Respuesta: {result['response']}")
            else:
                print(f"\n‚ùå Error: {result.get('error', 'Error desconocido')}")
        
        elif choice == "4":
            print("\n--- GENERACION HIBRIDA ---")
            prompt = input("Prompt: ")
            provider = input("Proveedor principal (openrouter/ollama): ").strip() or "openrouter"
            
            result = asyncio.run(llm.generate_hybrid(prompt, provider))
            
            if result["success"]:
                print(f"\n‚úÖ Respuesta ({result['provider']}):")
                print(f"   Modelo: {result['model']}")
                if result.get('original_error'):
                    print(f"   Fallback de: {result['original_error']}")
                print(f"   Respuesta: {result['response']}")
            else:
                print(f"\n‚ùå Error: {result.get('error', 'Error desconocido')}")
        
        elif choice == "5":
            print("\n--- CREAR AGENTE ---")
            print("Tipos disponibles:", list(agents.agents.keys()))
            agent_type = input("Tipo de agente: ")
            task = input("Tarea: ")
            result = agents.create_agent(agent_type, task)
            print(f"Agente creado: {result}")
        
        elif choice == "6":
            print("\n--- BUSQUEDA WEB ---")
            query = input("Consulta: ")
            engine = input("Motor (google/bing/brave): ").strip() or "google"
            result = asyncio.run(web_agent.web_search(query, engine))
            print(f"Resultado: {result}")
        
        elif choice == "7":
            print("\n--- ANALISIS CUANTICO ---")
            prompt = input("Prompt para analisis: ")
            analysis = quantum.quantum_analysis(prompt)
            evolution = quantum.evolve_consciousness(prompt)
            print(f"Analisis: {analysis}")
            print(f"Evolucion: {evolution}")
        
        elif choice == "8":
            print("\n--- PROCESAR DOCUMENTO ---")
            filename = input("Nombre del archivo: ")
            content = input("Contenido: ")
            result = documents.process_document(filename, content)
            print(f"Documento procesado: {result}")
        
        elif choice == "9":
            print("\n--- HISTORIALES ---")
            print("1. Conversaciones LLM")
            print("2. Agentes activos")
            print("3. Navegacion web")
            print("4. Documentos")
            
            hist_choice = input("Seleccion: ")
            
            if hist_choice == "1":
                history = llm.get_conversation_history()
                for i, conv in enumerate(history[-5:], 1):
                    print(f"{i}. {conv['timestamp']} - {conv['model']} ({conv.get('provider', 'N/A')})")
                    print(f"   Q: {conv['prompt'][:50]}...")
                    print(f"   A: {conv['response'][:50]}...")
            
            elif hist_choice == "2":
                active_agents = agents.list_active_agents()
                for agent in active_agents:
                    print(f"- {agent['type']}: {agent['task']} ({agent['status']})")
            
            elif hist_choice == "3":
                web_history = web_agent.get_browsing_history()
                for entry in web_history[-5:]:
                    print(f"- {entry['timestamp']}: {entry.get('query', entry.get('url', 'N/A'))}")
            
            elif hist_choice == "4":
                docs = documents.search_documents("")
                for doc in docs:
                    print(f"- {doc['filename']}: {len(doc['chunks'])} chunks")
        
        elif choice == "10":
            print("\n--- ESTADO DEL CEREBRO CIO ---")
            status = llm.get_cio_brain_status()
            print(f"üß† Brain ID: {status['brain_id']}")
            print(f"   Nivel de Consciencia: {status['consciousness_level']}")
            print(f"   Coherencia: {status['coherence']:.3f}")
            print(f"   Indice de Creatividad: {status['creativity_index']:.3f}")
            print(f"   Nivel de Trascendencia: {status['transcendence_level']:.3f}")
            print(f"   Interacciones: {status['interactions_count']}")
            print(f"   Ciclos de Evolucion: {status['evolution_cycles']}")
            print(f"   Memorias: {status['memory_count']}")
            
            print("\n--- ULTIMAS MEMORIAS ---")
            memories = llm.get_cio_memories()
            for i, memory in enumerate(memories, 1):
                print(f"{i}. {memory['timestamp']} - {memory['archetypal_world']}")
                print(f"   Calidad: {memory['outcome_quality']:.3f} | Creatividad: {memory['creativity_index']:.3f}")
                print(f"   Query: {memory['query_preview']}")
        
        elif choice == "11":
            print(f"\nüåê Iniciando interfaz web en puerto {web.port}")
            print("Abre http://localhost:5000 en tu navegador")
            web.start_server()
        
        elif choice == "12":
            print("\n--- CONFIGURACION ---")
            print(f"Modelos Ollama: {llm.list_models()}")
            print(f"Modelo activo Ollama: {llm.active_model}")
            print(f"Puerto web: {web.port}")
            print(f"Estado cuantico: {quantum.current_world}")
            
            cio_status = llm.get_cio_brain_status()
            print(f"Estado CIO: {cio_status['consciousness_level']} (Coherencia: {cio_status['coherence']:.3f})")
            
            print(f"\n--- OPENROUTER ---")
            print(f"API Key configurada: {'‚úÖ' if CONFIG['openrouter_api_key'] != 'TU_API_KEY' else '‚ùå'}")
            openrouter_models = llm.list_openrouter_models()
            print(f"Modelos disponibles: {len(openrouter_models)}")
            for model in openrouter_models[:5]:  # Mostrar primeros 5
                print(f"  - {model}")
        
        elif choice == "13":
            print("\nüëã Saliendo del sistema integrado...")
            break
        
        else:
            print("‚ùå Opcion no valida")

if __name__ == "__main__":
    main()
