#!/usr/bin/env python3
"""
üèÜ INDUSTRY BENCHMARK SYSTEM
Sistema para medirnos contra los mejores de la industria
"""

import asyncio
import aiohttp
import time
import json
from typing import Dict, Any, List
import statistics

class IndustryBenchmarkSystem:
    """Sistema de benchmark contra los mejores de la industria"""
    
    def __init__(self):
        self.api_key = "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"
        self.url = "https://openrouter.ai/api/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://industry-benchmark.local",
            "X-Title": "Industry Benchmark System"
        }
        
        # üèÜ MODELOS TOP DE LA INDUSTRIA
        self.industry_models = {
            "claude_opus": "anthropic/claude-3-5-sonnet",
            "gpt4o": "openai/gpt-4o",
            "deepseek_v3": "deepseek/deepseek-chat-v3.1",
            "gemini_pro": "google/gemini-2.5-pro",
            "mistral_medium": "mistralai/mistral-medium-3.1",
            "our_system": "google/gemini-flash-1.5-8b"  # Nuestro sistema base
        }
        
        # üìä M√©tricas de benchmark
        self.benchmark_results = {}
        
        print("üèÜ Industry Benchmark System inicializado")
        print("üéØ Objetivo: Medirnos contra los mejores de la industria")
    
    async def _call_model_benchmark(self, query: str, model: str) -> Dict[str, Any]:
        """Llama a un modelo para benchmark"""
        
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": query}],
            "max_tokens": 2000,
            "temperature": 0.1
        }
        
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.url,
                    headers=self.headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        content = data['choices'][0]['message']['content']
                        usage = data.get('usage', {})
                        
                        input_tokens = usage.get('prompt_tokens', 0)
                        output_tokens = usage.get('completion_tokens', 0)
                        
                        # Costos por modelo
                        cost_rates = {
                            "anthropic/claude-3-5-sonnet": (0.003, 0.015),
                            "openai/gpt-4o": (0.005, 0.015),
                            "deepseek/deepseek-chat-v3.1": (0.0014, 0.0028),
                            "google/gemini-2.5-pro": (0.00125, 0.01),
                            "mistralai/mistral-medium-3.1": (0.0007, 0.0028),
                            "google/gemini-flash-1.5-8b": (0.0000000375, 0.00000015)
                        }
                        
                        input_rate, output_rate = cost_rates.get(model, (0.001, 0.002))
                        cost = (input_tokens * input_rate / 1000000) + (output_tokens * output_rate / 1000000)
                        
                        response_time = time.time() - start_time
                        
                        return {
                            "success": True,
                            "response": content,
                            "cost": cost,
                            "response_time": response_time,
                            "input_tokens": input_tokens,
                            "output_tokens": output_tokens,
                            "model": model
                        }
                    else:
                        error_text = await response.text()
                        return {
                            "success": False,
                            "error": f"HTTP {response.status}: {error_text}",
                            "cost": 0.0,
                            "response_time": time.time() - start_time,
                            "model": model
                        }
                        
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "cost": 0.0,
                "response_time": time.time() - start_time,
                "model": model
            }
    
    def _evaluate_response_quality(self, response: str) -> Dict[str, float]:
        """Eval√∫a la calidad de una respuesta"""
        
        quality_metrics = {
            "completeness": 0.0,
            "accuracy": 0.0,
            "structure": 0.0,
            "code_quality": 0.0,
            "documentation": 0.0
        }
        
        # Evaluar completitud
        if len(response) > 500:
            quality_metrics["completeness"] = min(1.0, len(response) / 2000)
        
        # Evaluar estructura
        if "```" in response:
            quality_metrics["structure"] += 0.3
        if "#" in response or "##" in response:
            quality_metrics["structure"] += 0.2
        if "1." in response or "2." in response:
            quality_metrics["structure"] += 0.2
        if "def " in response or "class " in response:
            quality_metrics["structure"] += 0.3
        
        # Evaluar calidad de c√≥digo
        if "def " in response:
            quality_metrics["code_quality"] += 0.3
        if "class " in response:
            quality_metrics["code_quality"] += 0.2
        if "try:" in response or "except:" in response:
            quality_metrics["code_quality"] += 0.2
        if "import " in response:
            quality_metrics["code_quality"] += 0.1
        if "return " in response:
            quality_metrics["code_quality"] += 0.1
        if "assert " in response:
            quality_metrics["code_quality"] += 0.1
        
        # Evaluar documentaci√≥n
        if '"""' in response or "'''" in response:
            quality_metrics["documentation"] += 0.4
        if "#" in response:
            quality_metrics["documentation"] += 0.3
        if "Args:" in response or "Returns:" in response:
            quality_metrics["documentation"] += 0.3
        
        # Evaluar precisi√≥n (simplificado)
        quality_metrics["accuracy"] = 0.8  # Base assumption
        
        return quality_metrics
    
    async def run_benchmark(self, query: str) -> Dict[str, Any]:
        """Ejecuta benchmark completo"""
        
        print(f"\nüèÜ EJECUTANDO BENCHMARK")
        print(f"üìù Query: {query[:100]}...")
        
        results = {}
        
        # Probar todos los modelos
        for model_name, model_id in self.industry_models.items():
            print(f"üîÑ Probando {model_name}...")
            
            result = await self._call_model_benchmark(query, model_id)
            
            if result["success"]:
                quality = self._evaluate_response_quality(result["response"])
                results[model_name] = {
                    "success": True,
                    "cost": result["cost"],
                    "response_time": result["response_time"],
                    "input_tokens": result["input_tokens"],
                    "output_tokens": result["output_tokens"],
                    "quality_metrics": quality,
                    "overall_score": sum(quality.values()) / len(quality)
                }
                print(f"‚úÖ {model_name}: ${result['cost']:.8f}, {result['response_time']:.2f}s")
            else:
                results[model_name] = {
                    "success": False,
                    "error": result["error"],
                    "cost": 0.0,
                    "response_time": 0.0,
                    "overall_score": 0.0
                }
                print(f"‚ùå {model_name}: Error")
        
        return results
    
    def analyze_benchmark_results(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Analiza los resultados del benchmark"""
        
        successful_models = {k: v for k, v in results.items() if v["success"]}
        
        if not successful_models:
            return {"error": "No successful models"}
        
        # Calcular m√©tricas
        costs = [v["cost"] for v in successful_models.values()]
        times = [v["response_time"] for v in successful_models.values()]
        scores = [v["overall_score"] for v in successful_models.values()]
        
        analysis = {
            "total_models": len(results),
            "successful_models": len(successful_models),
            "success_rate": len(successful_models) / len(results) * 100,
            
            # Costos
            "min_cost": min(costs),
            "max_cost": max(costs),
            "avg_cost": statistics.mean(costs),
            "median_cost": statistics.median(costs),
            
            # Tiempos
            "min_time": min(times),
            "max_time": max(times),
            "avg_time": statistics.mean(times),
            "median_time": statistics.median(times),
            
            # Calidad
            "min_score": min(scores),
            "max_score": max(scores),
            "avg_score": statistics.mean(scores),
            "median_score": statistics.median(scores),
            
            # Rankings
            "cost_ranking": sorted(successful_models.items(), key=lambda x: x[1]["cost"]),
            "time_ranking": sorted(successful_models.items(), key=lambda x: x[1]["response_time"]),
            "quality_ranking": sorted(successful_models.items(), key=lambda x: x[1]["overall_score"], reverse=True),
            
            # Nuestro sistema
            "our_system_performance": successful_models.get("our_system", {}),
            "our_system_cost_rank": None,
            "our_system_time_rank": None,
            "our_system_quality_rank": None
        }
        
        # Calcular rankings de nuestro sistema
        if "our_system" in successful_models:
            cost_ranks = [i for i, (name, _) in enumerate(analysis["cost_ranking"]) if name == "our_system"]
            time_ranks = [i for i, (name, _) in enumerate(analysis["time_ranking"]) if name == "our_system"]
            quality_ranks = [i for i, (name, _) in enumerate(analysis["quality_ranking"]) if name == "our_system"]
            
            analysis["our_system_cost_rank"] = cost_ranks[0] + 1 if cost_ranks else None
            analysis["our_system_time_rank"] = time_ranks[0] + 1 if time_ranks else None
            analysis["our_system_quality_rank"] = quality_ranks[0] + 1 if quality_ranks else None
        
        return analysis
    
    def print_benchmark_analysis(self, analysis: Dict[str, Any]):
        """Imprime an√°lisis del benchmark"""
        
        print(f"\nüìä AN√ÅLISIS DE BENCHMARK")
        print("=" * 60)
        
        print(f"üéØ Modelos totales: {analysis['total_models']}")
        print(f"‚úÖ Exitosos: {analysis['successful_models']}")
        print(f"üìà Tasa de √©xito: {analysis['success_rate']:.1f}%")
        
        print(f"\nüí∞ AN√ÅLISIS DE COSTOS:")
        print(f"   M√≠nimo: ${analysis['min_cost']:.8f}")
        print(f"   M√°ximo: ${analysis['max_cost']:.8f}")
        print(f"   Promedio: ${analysis['avg_cost']:.8f}")
        print(f"   Mediana: ${analysis['median_cost']:.8f}")
        
        print(f"\n‚è±Ô∏è  AN√ÅLISIS DE TIEMPOS:")
        print(f"   M√≠nimo: {analysis['min_time']:.2f}s")
        print(f"   M√°ximo: {analysis['max_time']:.2f}s")
        print(f"   Promedio: {analysis['avg_time']:.2f}s")
        print(f"   Mediana: {analysis['median_time']:.2f}s")
        
        print(f"\nüéØ AN√ÅLISIS DE CALIDAD:")
        print(f"   M√≠nimo: {analysis['min_score']:.3f}")
        print(f"   M√°ximo: {analysis['max_score']:.3f}")
        print(f"   Promedio: {analysis['avg_score']:.3f}")
        print(f"   Mediana: {analysis['median_score']:.3f}")
        
        print(f"\nüèÜ RANKINGS:")
        print(f"   Costo (m√°s barato primero):")
        for i, (name, data) in enumerate(analysis["cost_ranking"], 1):
            print(f"     {i}. {name}: ${data['cost']:.8f}")
        
        print(f"   Tiempo (m√°s r√°pido primero):")
        for i, (name, data) in enumerate(analysis["time_ranking"], 1):
            print(f"     {i}. {name}: {data['response_time']:.2f}s")
        
        print(f"   Calidad (mejor primero):")
        for i, (name, data) in enumerate(analysis["quality_ranking"], 1):
            print(f"     {i}. {name}: {data['overall_score']:.3f}")
        
        if analysis["our_system_performance"]:
            print(f"\nüéØ NUESTRO SISTEMA:")
            print(f"   Costo: ${analysis['our_system_performance']['cost']:.8f} (Rank #{analysis['our_system_cost_rank']})")
            print(f"   Tiempo: {analysis['our_system_performance']['response_time']:.2f}s (Rank #{analysis['our_system_time_rank']})")
            print(f"   Calidad: {analysis['our_system_performance']['overall_score']:.3f} (Rank #{analysis['our_system_quality_rank']})")
            
            # An√°lisis competitivo
            cost_advantage = analysis["max_cost"] / analysis["our_system_performance"]["cost"] if analysis["our_system_performance"]["cost"] > 0 else 0
            time_advantage = analysis["avg_time"] / analysis["our_system_performance"]["response_time"] if analysis["our_system_performance"]["response_time"] > 0 else 0
            
            print(f"\nüöÄ VENTAJAS COMPETITIVAS:")
            print(f"   Ventaja de costo: {cost_advantage:.1f}x m√°s barato que el promedio")
            print(f"   Ventaja de tiempo: {time_advantage:.1f}x m√°s r√°pido que el promedio")

async def main():
    """Funci√≥n principal"""
    
    print("üöÄ INICIANDO INDUSTRY BENCHMARK SYSTEM")
    print("üèÜ OBJETIVO: MEDIRNOS CONTRA LOS MEJORES DE LA INDUSTRIA")
    print("=" * 70)
    
    benchmark_system = IndustryBenchmarkSystem()
    
    # Consultas de benchmark
    benchmark_queries = [
        "Implementa un algoritmo de ordenamiento quicksort optimizado con manejo de casos edge y an√°lisis de complejidad.",
        "Dise√±a un sistema de microservicios para una aplicaci√≥n de e-commerce con patrones de resiliencia y escalabilidad.",
        "Optimiza este c√≥digo Python para m√°xima eficiencia: def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)"
    ]
    
    all_results = {}
    
    for i, query in enumerate(benchmark_queries, 1):
        print(f"\nüéØ BENCHMARK {i}/{len(benchmark_queries)}")
        print("-" * 50)
        
        results = await benchmark_system.run_benchmark(query)
        analysis = benchmark_system.analyze_benchmark_results(results)
        benchmark_system.print_benchmark_analysis(analysis)
        
        all_results[f"benchmark_{i}"] = {
            "query": query,
            "results": results,
            "analysis": analysis
        }
    
    # An√°lisis final consolidado
    print(f"\nüèÜ AN√ÅLISIS FINAL CONSOLIDADO")
    print("=" * 70)
    
    # Calcular promedios de rankings
    our_cost_ranks = []
    our_time_ranks = []
    our_quality_ranks = []
    
    for benchmark_data in all_results.values():
        analysis = benchmark_data["analysis"]
        if analysis["our_system_cost_rank"]:
            our_cost_ranks.append(analysis["our_system_cost_rank"])
        if analysis["our_system_time_rank"]:
            our_time_ranks.append(analysis["our_system_time_rank"])
        if analysis["our_system_quality_rank"]:
            our_quality_ranks.append(analysis["our_system_quality_rank"])
    
    if our_cost_ranks:
        print(f"üí∞ Ranking promedio de costo: {statistics.mean(our_cost_ranks):.1f}")
    if our_time_ranks:
        print(f"‚è±Ô∏è  Ranking promedio de tiempo: {statistics.mean(our_time_ranks):.1f}")
    if our_quality_ranks:
        print(f"üéØ Ranking promedio de calidad: {statistics.mean(our_quality_ranks):.1f}")
    
    print(f"\nüèÜ INDUSTRY BENCHMARK COMPLETADO")
    print("üéØ Posici√≥n real determinada")

if __name__ == "__main__":
    asyncio.run(main())
