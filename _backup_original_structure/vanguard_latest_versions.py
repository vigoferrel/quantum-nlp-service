#!/usr/bin/env python3
"""
üèÜ VANGUARD PREMIUM ESSENCE SYSTEM 2025 - √öLTIMAS VERSIONES
Sistema optimizado con manejo robusto de rate limits:

‚úÖ CORRECCIONES PARA ERROR 429:
- Rate limiting inteligente con delays din√°micos
- Rotaci√≥n autom√°tica de modelos
- Colas de espera con exponential backoff
- Fallback a modelos gratuitos cuando sea necesario
- Cache agresivo para reducir llamadas a API

üöÄ √öLTIMAS VERSIONES 2025:
- GPT-5: M√°xima inteligencia (AIME 94.6%)
- Kimi-K2-Instruct: L√≠der en c√≥digo (SWE-bench 65.8%)
- Gemini 2.5 Flash-Lite: Velocidad (385 t/s)
- Claude Sonnet 4: Computer Use + 64K tokens
- DeepSeek V3.1: Precio-rendimiento ($0.14/$0.28)
"""

import asyncio
import aiohttp
import json
import time
import random
import hashlib
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict
import threading

@dataclass
class RateLimitInfo:
    """Informaci√≥n de rate limiting por modelo"""
    model: str
    last_request: float
    request_count: int
    rate_limit_window: int = 60  # segundos
    max_requests: int = 10  # requests por ventana

class VanguardLatestVersionsSystem:
    """Sistema con manejo robusto de rate limits y √∫ltimas versiones"""
    
    def __init__(self):
        self.api_key = "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"
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://vanguard-latest-versions-2025.local",
            "X-Title": "VANGUARD LATEST VERSIONS SYSTEM 2025"
        }
        
        # üèÜ MODELOS CON RATE LIMITING INTELIGENTE - √öLTIMAS VERSIONES 2025
        self.available_models = {
            # üèÜ L√çDERES POR CATEGOR√çA
            "kimi_k2": "moonshotai/kimi-k2:free",                    # ü•á CODIFICACI√ìN: 65.8% SWE-bench
            "gpt5": "openai/gpt-5",                                  # ü•á INTELIGENCIA: AIME 94.6%
            "claude_sonnet4": "anthropic/claude-3-5-sonnet",        # ü•á EMPRESARIAL: Computer Use + 64K tokens
            "gemini25_flash_lite": "google/gemini-2.5-flash-lite",   # ü•á VELOCIDAD: 385 t/s + $0.10/$0.40
            "deepseek_v31": "deepseek/deepseek-chat-v3.1",          # ü•á PRECIO: $0.14/$0.28 + Open source
            "mistral_medium31": "mistralai/mistral-medium-3.1",      # ü•á PRIVACIDAD: On-premise + GDPR
            "gpt41": "openai/gpt-4.1",                              # ü•á CONTEXTO: 1M tokens
            "gemini20_flash": "google/gemini-2.0-flash-001",        # ü•á PRECISI√ìN: 0.7% alucinaci√≥n
        }
        
        # üåå MODELOS GRATUITOS COMO FALLBACK - √öLTIMAS VERSIONES
        self.free_models = {
            "kimi_k2": "moonshotai/kimi-k2:free",                    # ü•á Gratuito l√≠der en c√≥digo
            "qwen3_coder": "qwen/qwen3-coder:free",                  # ü•à Gratuito especializado en c√≥digo
            "deepseek_chimera": "tngtech/deepseek-r1t2-chimera:free", # ü•â Gratuito h√≠brido
            "mistral_small": "mistralai/mistral-small-3.2-24b-instruct:free", # üèÖ Gratuito local
        }
        
        # üìä RATE LIMITING POR MODELO
        self.rate_limit_tracker = {}
        self.rate_limit_lock = threading.Lock()
        
        # üß† CACHE AGRESIVO PARA REDUCIR LLAMADAS
        self.essence_cache = {}
        self.markov_cache = {}
        
        # üìä M√âTRICAS CON RATE LIMITING
        self.cost_tracker = {
            "total_spent": 0.0,
            "premium_extractions": 0,
            "free_model_uses": 0,
            "cache_hits": 0,
            "rate_limit_hits": 0,
            "model_rotations": 0,
            "fallback_uses": 0,
            "total_delay_time": 0.0,
            "markov_generations": 0,
            "top_model_uses": 0,
            "retry_attempts": 0,
            "quality_achieved": 0.0
        }
        
        # üèÜ RANKINGS ACTUALIZADOS 2025
        self.optimized_rankings = {
            "programming": {
                "kimi_k2": {"position": 1, "score": 65.8, "benchmark": "SWE-bench"},
                "gpt5": {"position": 2, "score": 72.5, "benchmark": "SWE-bench"},
                "deepseek_v31": {"position": 3, "score": 50.0, "benchmark": "SWE-bench"},
                "claude_sonnet4": {"position": 4, "score": 49.0, "benchmark": "SWE-bench"},
                "gpt41": {"position": 5, "score": 54.6, "benchmark": "SWE-bench"}
            },
            "mathematics": {
                "gpt5": {"position": 1, "score": 94.6, "benchmark": "AIME 2025"},
                "claude_sonnet4": {"position": 2, "score": 85.0, "benchmark": "AIME 2025"},
                "deepseek_v31": {"position": 3, "score": 82.0, "benchmark": "AIME 2025"},
                "gemini25_flash_lite": {"position": 4, "score": 78.0, "benchmark": "AIME 2025"},
                "gpt41": {"position": 5, "score": 75.0, "benchmark": "AIME 2025"}
            },
            "science": {
                "gpt5": {"position": 1, "score": 92.0, "benchmark": "GPQA-Diamond"},
                "claude_sonnet4": {"position": 2, "score": 88.0, "benchmark": "GPQA-Diamond"},
                "gemini20_flash": {"position": 3, "score": 85.0, "benchmark": "GPQA-Diamond"},
                "deepseek_v31": {"position": 4, "score": 82.0, "benchmark": "GPQA-Diamond"},
                "gpt41": {"position": 5, "score": 80.0, "benchmark": "GPQA-Diamond"}
            }
        }
        
        print("üèÜ VANGUARD PREMIUM ESSENCE SYSTEM 2025 - √öLTIMAS VERSIONES")
        print("‚úÖ Manejo robusto de rate limits implementado")
        print("üîÑ Rotaci√≥n autom√°tica de modelos")
        print("üíæ Cache agresivo para reducir llamadas")
        print("üöÄ Usando modelos m√°s actualizados: GPT-5, Kimi-K2-Instruct, Gemini 2.5")

    def _check_rate_limit(self, model: str) -> Tuple[bool, float]:
        """Verifica si el modelo est√° en rate limit"""
        with self.rate_limit_lock:
            current_time = time.time()
            
            if model not in self.rate_limit_tracker:
                self.rate_limit_tracker[model] = RateLimitInfo(model, 0, 0)
            
            rate_info = self.rate_limit_tracker[model]
            
            # Verificar si estamos en la misma ventana de tiempo
            if current_time - rate_info.last_request < rate_info.rate_limit_window:
                if rate_info.request_count >= rate_info.max_requests:
                    # En rate limit, calcular tiempo de espera
                    wait_time = rate_info.rate_limit_window - (current_time - rate_info.last_request)
                    return False, wait_time
            else:
                # Nueva ventana, resetear contador
                rate_info.request_count = 0
                rate_info.last_request = current_time
            
            # Incrementar contador
            rate_info.request_count += 1
            return True, 0.0

    def _update_rate_limit(self, model: str, success: bool):
        """Actualiza informaci√≥n de rate limiting"""
        with self.rate_limit_lock:
            if model in self.rate_limit_tracker:
                if not success:
                    # Si fall√≥, reducir el l√≠mite temporalmente
                    self.rate_limit_tracker[model].max_requests = max(1, self.rate_limit_tracker[model].max_requests - 1)

    async def _make_rate_limited_api_call(self, model: str, prompt: str, max_tokens: int = 1500) -> Tuple[bool, str, float]:
        """Hace llamada a API con manejo de rate limits"""
        max_retries = 5
        
        for attempt in range(max_retries):
            # Verificar rate limit
            can_proceed, wait_time = self._check_rate_limit(model)
            
            if not can_proceed:
                print(f"‚è≥ Rate limit detectado para {model}, esperando {wait_time:.1f}s...")
                self.cost_tracker["rate_limit_hits"] += 1
                self.cost_tracker["total_delay_time"] += wait_time
                await asyncio.sleep(wait_time + 1)  # +1 segundo extra de seguridad
            
            try:
                async with aiohttp.ClientSession() as session:
                    response = await session.post(
                        self.base_url,
                        headers=self.headers,
                        json={
                            "model": model,
                            "messages": [{"role": "user", "content": prompt}],
                            "max_tokens": max_tokens,
                            "temperature": 0.3
                        },
                        timeout=30
                    )
                    
                    if response.status == 200:
                        data = await response.json()
                        content = data['choices'][0]['message']['content']
                        
                        # Calcular costo
                        input_tokens = len(prompt.split()) * 1.3
                        output_tokens = len(content.split()) * 1.3
                        
                        cost_rates = {
                            # üèÜ √öLTIMAS VERSIONES 2025 - PRECIOS ACTUALIZADOS
                            "openai/gpt-5": (0.005, 0.015),                    # GPT-5: M√°xima inteligencia
                            "openai/gpt-4.1": (0.003, 0.015),                  # GPT-4.1: 1M contexto
                            "anthropic/claude-3-5-sonnet": (0.003, 0.015),     # Claude Sonnet 4: Computer Use
                            "google/gemini-2.5-flash-lite": (0.0001, 0.0004),  # Gemini 2.5: 385 t/s + $0.10/$0.40
                            "google/gemini-2.0-flash-001": (0.000125, 0.0005), # Gemini 2.0: 0.7% alucinaci√≥n
                            "deepseek/deepseek-chat-v3.1": (0.00014, 0.00028), # DeepSeek: $0.14/$0.28
                            "mistralai/mistral-medium-3.1": (0.0004, 0.002),   # Mistral: On-premise
                            "moonshotai/kimi-k2:free": (0.0, 0.0),             # Kimi-K2: Gratuito l√≠der
                            "qwen/qwen3-coder:free": (0.0, 0.0),               # Qwen3: Gratuito c√≥digo
                            "tngtech/deepseek-r1t2-chimera:free": (0.0, 0.0),  # Chimera: Gratuito h√≠brido
                            "mistralai/mistral-small-3.2-24b-instruct:free": (0.0, 0.0) # Mistral: Gratuito local
                        }
                        
                        input_cost, output_cost = cost_rates.get(model, (0.001, 0.002))
                        total_cost = (input_tokens * input_cost / 1000000) + (output_tokens * output_cost / 1000000)
                        
                        self._update_rate_limit(model, True)
                        return True, content, total_cost
                        
                    elif response.status == 429:
                        print(f"‚ö†Ô∏è Rate limit 429 para {model}, intento {attempt + 1}")
                        self._update_rate_limit(model, False)
                        
                        if attempt < max_retries - 1:
                            wait_time = (2 ** attempt) + random.uniform(1, 3)  # Exponential backoff + jitter
                            print(f"‚è≥ Esperando {wait_time:.1f}s antes de reintentar...")
                            self.cost_tracker["total_delay_time"] += wait_time
                            await asyncio.sleep(wait_time)
                        else:
                            return False, f"Rate limit persistente para {model}", 0.0
                            
                    else:
                        print(f"‚ö†Ô∏è Error {response.status} para {model}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)
                        else:
                            return False, f"Error {response.status} despu√©s de {max_retries} intentos", 0.0
                            
            except Exception as e:
                print(f"‚ö†Ô∏è Error de conexi√≥n para {model}: {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return False, f"Error de conexi√≥n: {str(e)}", 0.0
        
        return False, "M√°ximo de reintentos excedido", 0.0

    def _generate_robust_markov_chain(self, response: str) -> Dict[str, List[str]]:
        """Genera cadena de Markov ROBUSTA (corregida)"""
        if not response or len(response.strip()) < 50:
            # Fallback si la respuesta es muy corta
            return {"default": ["response", "generated", "successfully"]}
        
        # Limpiar y preparar texto
        words = response.replace('\n', ' ').replace('\t', ' ').split()
        words = [word.strip().lower() for word in words if len(word.strip()) > 2]
        
        if len(words) < 10:
            # Fallback si no hay suficientes palabras
            return {"default": ["content", "processed", "effectively"]}
        
        markov_chain = defaultdict(list)
        
        # Construir cadena de Markov robusta
        for i in range(len(words) - 1):
            current_word = words[i]
            next_word = words[i + 1]
            markov_chain[current_word].append(next_word)
        
        # Optimizar y limpiar la cadena
        optimized_chain = {}
        for word, next_words in markov_chain.items():
            if len(next_words) > 0:
                # Contar frecuencias
                word_counts = defaultdict(int)
                for next_word in next_words:
                    word_counts[next_word] += 1
                
                # Ordenar por frecuencia y tomar top 5
                sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
                top_words = [word for word, count in sorted_words[:5] if len(word) > 2]
                
                if top_words:
                    optimized_chain[word] = top_words
        
        # Asegurar que la cadena no est√© vac√≠a
        if not optimized_chain:
            optimized_chain = {
                "system": ["optimized", "successfully"],
                "response": ["generated", "effectively"],
                "quality": ["improved", "significantly"]
            }
        
        return optimized_chain

    def _extract_optimized_patterns(self, response: str, category: str, ranking_data: Dict) -> List[str]:
        """Extrae patrones OPTIMIZADOS usando an√°lisis avanzado"""
        patterns = []
        
        # An√°lisis de estructura mejorado
        lines = response.split('\n')
        code_blocks = sum(1 for line in lines if '```' in line)
        numbered_lists = sum(1 for line in lines if line.strip() and line.strip()[0].isdigit() and '. ' in line)
        bullet_points = sum(1 for line in lines if line.strip().startswith(('- ', '* ', '‚Ä¢ ')))
        
        # An√°lisis de contenido por categor√≠a
        if category == "programming":
            if code_blocks > 0:
                patterns.append("Implementaci√≥n con bloques de c√≥digo estructurados")
            if "def " in response or "function " in response:
                patterns.append("Definici√≥n de funciones con documentaci√≥n")
            if "class " in response:
                patterns.append("Arquitectura orientada a objetos")
            if "import " in response or "require " in response:
                patterns.append("Gesti√≥n profesional de dependencias")
            if any(term in response.lower() for term in ["algorithm", "architecture", "optimization"]):
                patterns.append("Uso de terminolog√≠a t√©cnica avanzada")
            if numbered_lists > 2:
                patterns.append("Organizaci√≥n secuencial paso a paso")
        
        elif category == "mathematics":
            if any(symbol in response for symbol in ['=', '+', '-', '*', '/', '‚àë', '‚à´', '‚àö']):
                patterns.append("Expresiones matem√°ticas formales")
            if "theorem" in response.lower() or "proof" in response.lower():
                patterns.append("Demostraciones matem√°ticas rigurosas")
            if "algorithm" in response.lower():
                patterns.append("Algoritmos matem√°ticos optimizados")
            if numbered_lists > 2:
                patterns.append("Razonamiento matem√°tico estructurado")
            if "optimization" in response.lower():
                patterns.append("T√©cnicas de optimizaci√≥n matem√°tica")
        
        elif category == "science":
            if any(term in response.lower() for term in ["method", "experiment", "analysis"]):
                patterns.append("Metodolog√≠a cient√≠fica rigurosa")
            if "results" in response.lower() or "conclusion" in response.lower():
                patterns.append("An√°lisis de resultados detallado")
            if "hypothesis" in response.lower():
                patterns.append("Formulaci√≥n de hip√≥tesis cient√≠ficas")
            if bullet_points > 2:
                patterns.append("Organizaci√≥n de conceptos cient√≠ficos")
        
        # Patrones generales mejorados
        if len(response) > 800:
            patterns.append("Respuesta comprehensiva y detallada")
        if code_blocks > 1:
            patterns.append("M√∫ltiples ejemplos de implementaci√≥n")
        if any(marker in response for marker in ["##", "###", "1.", "2.", "‚Ä¢"]):
            patterns.append("Estructuraci√≥n clara y organizada")
        
        return patterns if patterns else ["Estructuraci√≥n premium general"]

    async def extract_optimized_premium_essence(self, category: str, question: str) -> Dict[str, Any]:
        """Extrae esencia premium OPTIMIZADA con fallback robusto"""
        cache_key = self._generate_cache_key(category, question)
        
        # Verificar cache
        if cache_key in self.essence_cache:
            self.cost_tracker["cache_hits"] += 1
            print(f"‚úÖ Cache hit para esencia premium: {category}")
            return self.essence_cache[cache_key]
        
        # Seleccionar mejor modelo disponible
        model_id, model = self._select_available_model_with_rotation(category)
        
        print(f"üîç Extrayendo esencia del modelo premium: {model_id}")
        
        # Obtener ranking data
        ranking_data = self.optimized_rankings.get(category, {}).get(model_id, {"position": 5, "score": 70.0, "benchmark": "General"})
        
        # Prompt OPTIMIZADO para mejor extracci√≥n
        optimized_prompt = f"""
        Eres {model_id}, un modelo premium especializado en {category}.
        Tu benchmark score es {ranking_data['score']}% en {ranking_data['benchmark']}.
        
        Responde a esta pregunta con M√ÅXIMA CALIDAD, demostrando:
        
        1. ESTRUCTURA CLARA:
           - Organizaci√≥n l√≥gica y secuencial
           - Uso de headers, listas numeradas y bullet points
           - Separaci√≥n clara de conceptos
        
        2. CONTENIDO T√âCNICO:
           - Terminolog√≠a espec√≠fica de {category}
           - Ejemplos concretos y aplicables
           - Explicaciones paso a paso detalladas
        
        3. CALIDAD PREMIUM:
           - An√°lisis profundo y comprehensivo
           - Consideraciones avanzadas y mejores pr√°cticas
           - Aplicabilidad pr√°ctica inmediata
        
        Pregunta: {question}
        
        Responde con calidad premium que justifique tu ranking #{ranking_data['position']} en {ranking_data['benchmark']}.
        """
        
        # Llamada robusta con retry logic
        success, api_response, cost = await self._make_rate_limited_api_call(model, optimized_prompt, max_tokens=2000)
        
        if success:
            # Extraer patrones optimizados
            patterns = self._extract_optimized_patterns(api_response, category, ranking_data)
            
            # Generar cadena de Markov robusta
            markov_chain = self._generate_robust_markov_chain(api_response)
            
            # Calcular boost de calidad optimizado
            quality_boost = self._calculate_optimized_quality_boost(api_response, category, ranking_data)
            
            # Crear esencia optimizada
            essence_data = {
                "model_name": model_id,
                "category": category,
                "patterns": patterns,
                "quality_boost": quality_boost,
                "cost_per_request": cost,
                "api_response": api_response,
                "markov_chain": markov_chain,
                "cache_key": cache_key,
                "success": True,
                "ranking_position": ranking_data['position'],
                "benchmark_score": ranking_data['score']
            }
            
            # Guardar en cache
            self.essence_cache[cache_key] = essence_data
            self.markov_cache[cache_key] = markov_chain
            
            # Actualizar m√©tricas
            self.cost_tracker["total_spent"] += cost
            self.cost_tracker["premium_extractions"] += 1
            self.cost_tracker["markov_generations"] += 1
            self.cost_tracker["top_model_uses"] += 1
            
            print(f"‚úÖ Esencia OPTIMIZADA extra√≠da de {model_id}")
            print(f"   üìä Patrones: {len(patterns)}")
            print(f"   üß† Cadena Markov: {len(markov_chain)} estados")
            print(f"   üí∞ Costo: ${cost:.6f}")
            print(f"   üèÜ Boost calidad: {quality_boost:.2f}x")
            
            return essence_data
        else:
            print(f"‚ùå Error en extracci√≥n: {api_response}")
            self.cost_tracker["fallback_uses"] += 1
            return {
                "model_name": model_id,
                "category": category,
                "patterns": ["Estructuraci√≥n optimizada", "An√°lisis general mejorado"],
                "quality_boost": 1.5,
                "cost_per_request": 0.0,
                "api_response": "Optimized fallback response",
                "markov_chain": {"optimized": ["fallback", "response", "generated"]},
                "cache_key": cache_key,
                "success": False,
                "ranking_position": 5,
                "benchmark_score": 70.0
            }

    def _select_available_model_with_rotation(self, category: str) -> Tuple[str, str]:
        """Selecciona modelo disponible con rotaci√≥n para evitar rate limits"""
        # Modelos por categor√≠a con prioridad - √öLTIMAS VERSIONES 2025
        category_models = {
            "programming": ["kimi_k2", "gpt5", "deepseek_v31", "claude_sonnet4"],  # ü•á Kimi-K2: 65.8% SWE-bench
            "mathematics": ["gpt5", "claude_sonnet4", "deepseek_v31", "gemini25_flash_lite"], # ü•á GPT-5: 94.6% AIME
            "science": ["gpt5", "claude_sonnet4", "gemini20_flash", "deepseek_v31"]  # ü•á GPT-5: 92% GPQA-Diamond
        }
        
        models = category_models.get(category, ["gpt5", "claude_sonnet4", "deepseek_v31"])
        
        # Verificar disponibilidad de cada modelo
        for model_id in models:
            if model_id in self.available_models:
                can_proceed, _ = self._check_rate_limit(self.available_models[model_id])
                if can_proceed:
                    return model_id, self.available_models[model_id]
        
        # Si todos est√°n en rate limit, usar modelo gratuito
        print("üîÑ Todos los modelos premium en rate limit, usando modelo gratuito")
        self.cost_tracker["model_rotations"] += 1
        return "qwen3_coder", self.free_models["qwen3_coder"]

    def _generate_cache_key(self, category: str, question: str) -> str:
        """Genera clave de cache optimizada"""
        content = f"LATEST_VERSIONS_{category}:{question[:100]}"
        return hashlib.md5(content.encode()).hexdigest()

    def _calculate_optimized_quality_boost(self, response: str, category: str, ranking_data: Dict) -> float:
        """Calcula boost de calidad OPTIMIZADO"""
        base_boost = 1.8  # Base mejorada
        
        # Factor de ranking
        ranking_factor = 1.0 + (1.0 / ranking_data['position'])
        
        # Factor de benchmark score
        score_factor = ranking_data['score'] / 100.0
        
        # Factores de calidad de respuesta
        length_factor = min(2.0, len(response) / 1200)
        structure_factor = 1.0
        technical_factor = 1.0
        
        # An√°lisis de estructura
        if "##" in response or "###" in response:
            structure_factor = 1.3
        if "```" in response:
            structure_factor = 1.4
        if any(marker in response for marker in ["1.", "2.", "3.", "‚Ä¢", "-"]):
            structure_factor = 1.2
        
        # An√°lisis t√©cnico
        technical_terms = {
            "programming": ["def ", "class ", "import ", "function ", "algorithm", "architecture"],
            "mathematics": ["theorem", "proof", "equation", "formula", "algorithm", "optimization"],
            "science": ["method", "experiment", "analysis", "results", "conclusion", "hypothesis"]
        }
        
        category_terms = technical_terms.get(category, [])
        technical_count = sum(1 for term in category_terms if term in response.lower())
        technical_factor = 1.0 + (technical_count * 0.1)
        
        # Calcular boost final
        final_boost = base_boost * ranking_factor * score_factor * length_factor * structure_factor * technical_factor
        return min(4.0, final_boost)

    def _apply_optimized_markov_transformation(self, question: str, essence: Dict[str, Any]) -> str:
        """Aplica transformaci√≥n Markov OPTIMIZADA"""
        if not essence['markov_chain']:
            essence['markov_chain'] = {"optimized": ["transformation", "applied", "successfully"]}
        
        # Generar secuencia usando cadena Markov optimizada
        words = question.split()
        if not words:
            words = ["optimized", "question"]
        
        current_word = words[0].lower()
        transformed_words = [current_word]
        
        # Generar secuencia optimizada
        for _ in range(min(60, len(words) * 2)):
            if current_word in essence['markov_chain']:
                next_words = essence['markov_chain'][current_word]
                if next_words:
                    current_word = random.choice(next_words)
                    transformed_words.append(current_word)
                else:
                    break
            else:
                break
        
        # Prompt transformado optimizado
        transformed_prompt = f"""
        {essence['model_name']} - TRANSFORMACI√ìN OPTIMIZADA
        
        CATEGOR√çA: {essence['category']}
        PATRONES: {', '.join(essence['patterns'])}
        BOOST: {essence['quality_boost']}x
        MARKOV: {len(essence['markov_chain'])} estados
        
        INSTRUCCIONES OPTIMIZADAS:
        - Aplica los patrones de {essence['model_name']}
        - Usa la estructura identificada
        - Mant√©n calidad premium
        - Optimiza para {essence['category']}
        
        PREGUNTA: {question}
        SECUENCIA: {' '.join(transformed_words[:10])}...
        
        Responde con calidad premium aplicando la esencia de {essence['model_name']}.
        """
        
        return transformed_prompt

    async def generate_optimized_response(self, question: str, category: str) -> Dict[str, Any]:
        """Genera respuesta OPTIMIZADA"""
        
        # 1. Extraer esencia optimizada
        essence = await self.extract_optimized_premium_essence(category, question)
        
        # 2. Aplicar transformaci√≥n optimizada
        enhanced_prompt = self._apply_optimized_markov_transformation(question, essence)
        
        # 3. Usar modelo gratuito con esencia aplicada - √öLTIMA VERSI√ìN
        free_model = "qwen3_coder"  # ü•à Qwen3: Gratuito especializado en c√≥digo
        
        # Llamada robusta al modelo gratuito
        success, response_content, free_cost = await self._make_rate_limited_api_call(
            self.free_models[free_model], 
            enhanced_prompt, 
            max_tokens=1500
        )
        
        if success:
            # Calcular calidad optimizada
            optimized_quality = self._calculate_optimized_response_quality(response_content, category, essence)
            
            # Actualizar m√©tricas
            self.cost_tracker["free_model_uses"] += 1
            self.cost_tracker["total_spent"] += free_cost
            self.cost_tracker["quality_achieved"] = optimized_quality
            
            return {
                "response": response_content,
                "model_used": free_model,
                "essence_applied": essence['model_name'],
                "quality_score": optimized_quality,
                "cost_optimization": "OPTIMIZED_ESENCIA_APLICADA",
                "cost_spent": essence['cost_per_request'] + free_cost,
                "markov_states": len(essence['markov_chain']),
                "cache_hit": essence['cache_key'] in self.essence_cache,
                "patterns_applied": len(essence['patterns']),
                "ranking_position": essence['ranking_position'],
                "benchmark_score": essence['benchmark_score'],
                "premium_boost": essence['quality_boost'],
                "retry_attempts": self.cost_tracker["retry_attempts"],
                "fallback_used": self.cost_tracker["fallback_uses"]
            }
        else:
            return {
                "response": f"Error: {response_content}",
                "model_used": "error",
                "essence_applied": essence['model_name'],
                "quality_score": 0.5,
                "cost_optimization": "ERROR",
                "cost_spent": essence['cost_per_request'],
                "markov_states": len(essence['markov_chain']),
                "cache_hit": False,
                "patterns_applied": len(essence['patterns']),
                "ranking_position": essence['ranking_position'],
                "benchmark_score": essence['benchmark_score'],
                "premium_boost": essence['quality_boost'],
                "retry_attempts": self.cost_tracker["retry_attempts"],
                "fallback_used": self.cost_tracker["fallback_uses"]
            }

    def _calculate_optimized_response_quality(self, response: str, category: str, essence: Dict[str, Any]) -> float:
        """Calcula calidad de respuesta OPTIMIZADA"""
        base_score = 0.65  # Base mejorada
        
        # Factores de mejora optimizados
        essence_boost = essence['quality_boost']
        pattern_match = 0.0
        structure_bonus = 0.0
        ranking_bonus = 0.0
        
        # Verificar aplicaci√≥n de patrones
        for pattern in essence['patterns']:
            if any(keyword in response.lower() for keyword in pattern.lower().split()):
                pattern_match += 0.06
        
        # Bonus por estructura
        if "```" in response:
            structure_bonus += 0.12
        if any(marker in response for marker in ["##", "###", "1.", "2.", "‚Ä¢"]):
            structure_bonus += 0.12
        if len(response) > 600:
            structure_bonus += 0.12
        
        # Bonus por ranking
        ranking_bonus = (1.0 / essence['ranking_position']) * 0.15
        
        # Calcular calidad final optimizada
        enhanced_score = min(1.0, base_score * essence_boost + pattern_match + structure_bonus + ranking_bonus)
        return round(enhanced_score, 3)

    async def test_latest_versions_system(self) -> Dict[str, Any]:
        """Prueba del sistema con √öLTIMAS VERSIONES"""
        print("\nüèÜ INICIANDO PRUEBA CON √öLTIMAS VERSIONES 2025")
        print("=" * 60)
        
        test_questions = {
            "programming": [
                "Implementa un sistema de microservicios con arquitectura hexagonal usando Spring Boot",
                "Crea un algoritmo de machine learning para detecci√≥n de anomal√≠as en tiempo real"
            ],
            "mathematics": [
                "Resuelve el problema de optimizaci√≥n combinatoria: Traveling Salesman Problem con 1000 ciudades",
                "Implementa un algoritmo de clustering jer√°rquico para an√°lisis de datos masivos"
            ],
            "science": [
                "Desarrolla un modelo de mec√°nica cu√°ntica para sistemas de m√∫ltiples part√≠culas",
                "Implementa algoritmos de computaci√≥n cu√°ntica para factorizaci√≥n de n√∫meros primos"
            ]
        }
        
        results = {
            "categories_tested": [],
            "quality_scores": [],
            "cost_analysis": {},
            "optimized_applications": [],
            "performance_metrics": {}
        }
        
        for category, questions in test_questions.items():
            print(f"\nüéØ Probando categor√≠a: {category}")
            
            for i, question in enumerate(questions[:1]):
                print(f"  üìù Pregunta {i+1}: {question[:80]}...")
                
                result = await self.generate_optimized_response(question, category)
                
                results["categories_tested"].append(category)
                results["quality_scores"].append(result["quality_score"])
                results["optimized_applications"].append({
                    "category": category,
                    "essence": result["essence_applied"],
                    "quality": result["quality_score"],
                    "cost": result["cost_spent"],
                    "markov_states": result["markov_states"],
                    "patterns_applied": result["patterns_applied"],
                    "cache_hit": result["cache_hit"],
                    "ranking_position": result["ranking_position"],
                    "benchmark_score": result["benchmark_score"],
                    "premium_boost": result["premium_boost"],
                    "retry_attempts": result["retry_attempts"],
                    "fallback_used": result["fallback_used"]
                })
                
                print(f"  ‚úÖ Esencia aplicada: {result['essence_applied']}")
                print(f"  üìä Calidad: {result['quality_score']}")
                print(f"  üí∞ Costo: ${result['cost_spent']:.6f}")
                print(f"  üß† Estados Markov: {result['markov_states']}")
                print(f"  üéØ Patrones aplicados: {result['patterns_applied']}")
                print(f"  üíæ Cache hit: {result['cache_hit']}")
                print(f"  üèÜ Ranking: #{result['ranking_position']} ({result['benchmark_score']}%)")
                print(f"  ‚ö° Boost: {result['premium_boost']:.2f}x")
                print(f"  üîÑ Reintentos: {result['retry_attempts']}")
                print(f"  üÜò Fallback: {result['fallback_used']}")
        
        # An√°lisis final optimizado
        results["cost_analysis"] = {
            "total_spent": self.cost_tracker["total_spent"],
            "premium_extractions": self.cost_tracker["premium_extractions"],
            "free_model_uses": self.cost_tracker["free_model_uses"],
            "cache_hits": self.cost_tracker["cache_hits"],
            "markov_generations": self.cost_tracker["markov_generations"],
            "top_model_uses": self.cost_tracker["top_model_uses"],
            "retry_attempts": self.cost_tracker["retry_attempts"],
            "fallback_uses": self.cost_tracker["fallback_uses"],
            "average_quality": sum(results["quality_scores"]) / len(results["quality_scores"]),
            "cost_per_quality": self.cost_tracker["total_spent"] / len(results["quality_scores"])
        }
        
        results["performance_metrics"] = {
            "cache_hit_rate": self.cost_tracker["cache_hits"] / max(1, self.cost_tracker["premium_extractions"]),
            "retry_rate": self.cost_tracker["retry_attempts"] / max(1, self.cost_tracker["premium_extractions"]),
            "fallback_rate": self.cost_tracker["fallback_uses"] / max(1, self.cost_tracker["premium_extractions"]),
            "essences_cached": len(self.essence_cache),
            "markov_chains_cached": len(self.markov_cache)
        }
        
        return results

    def print_latest_versions_summary(self, results: Dict[str, Any]):
        """Imprime resumen con √öLTIMAS VERSIONES"""
        print("\n" + "=" * 80)
        print("üèÜ RESUMEN CON √öLTIMAS VERSIONES 2025")
        print("=" * 80)
        
        cost_analysis = results["cost_analysis"]
        performance = results["performance_metrics"]
        
        print(f"\nüí∞ AN√ÅLISIS DE COSTOS:")
        print(f"  üí∞ Costo total: ${cost_analysis['total_spent']:.6f}")
        print(f"  üìä Calidad promedio: {cost_analysis['average_quality']:.3f}")
        print(f"  üíé Costo por calidad: ${cost_analysis['cost_per_quality']:.6f}")
        
        print(f"\nüèÜ MODELOS √öLTIMAS VERSIONES UTILIZADOS:")
        print(f"  ü•á Kimi-K2-Instruct: 65.8% SWE-bench (programming)")
        print(f"  ü•á GPT-5: 94.6% AIME 2025 (mathematics)")
        print(f"  ü•á GPT-5: 92% GPQA-Diamond (science)")
        print(f"  ü•á Gemini 2.5 Flash-Lite: 385 t/s velocidad")
        print(f"  ü•á Claude Sonnet 4: Computer Use + 64K tokens")
        print(f"  ü•á DeepSeek V3.1: $0.14/$0.28 precio-rendimiento")
        
        print(f"\nüíæ RENDIMIENTO DE CACHE:")
        print(f"  üìà Tasa de acierto: {performance['cache_hit_rate']:.1%}")
        print(f"  üóÑÔ∏è  Esencias cacheadas: {performance['essences_cached']}")
        print(f"  üß† Cadenas Markov: {performance['markov_chains_cached']}")
        
        print(f"\n‚ö†Ô∏è AN√ÅLISIS DE ERRORES:")
        print(f"  üîÑ Reintentos: {cost_analysis['retry_attempts']}")
        print(f"  üÜò Fallbacks: {cost_analysis['fallback_uses']}")
        print(f"  üìâ Tasa de error: {performance['retry_rate']:.1%}")
        
        print(f"\nüèÜ VEREDICTO FINAL:")
        if cost_analysis['average_quality'] >= 0.8:
            print(f"  üåü SISTEMA EXCEPCIONAL - √öltimas versiones funcionando")
        elif cost_analysis['average_quality'] >= 0.7:
            print(f"  ‚≠ê SISTEMA OPTIMIZADO - Objetivos alcanzados")
        elif cost_analysis['average_quality'] >= 0.6:
            print(f"  ‚úÖ SISTEMA FUNCIONAL - Mejoras necesarias")
        else:
            print(f"  üìà SISTEMA EN DESARROLLO - Optimizaci√≥n requerida")

async def main():
    """Funci√≥n principal con √öLTIMAS VERSIONES"""
    system = VanguardLatestVersionsSystem()
    
    try:
        print("üöÄ INICIANDO SISTEMA CON √öLTIMAS VERSIONES 2025...")
        results = await system.test_latest_versions_system()
        system.print_latest_versions_summary(results)
        
    except Exception as e:
        print(f"‚ùå Error en ejecuci√≥n: {e}")

if __name__ == "__main__":
    asyncio.run(main())
