#!/usr/bin/env python3
"""
üèÜ VANGUARD PREMIUM ESSENCE SYSTEM 2025 - RATE LIMIT FIXED
Sistema optimizado con manejo robusto de rate limits:

‚úÖ CORRECCIONES PARA ERROR 429:
- Rate limiting inteligente con delays din√°micos
- Rotaci√≥n autom√°tica de modelos
- Colas de espera con exponential backoff
- Fallback a modelos gratuitos cuando sea necesario
- Cache agresivo para reducir llamadas a API
"""

import asyncio
import aiohttp
import json
import time
import random
import hashlib
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict
import threading

@dataclass
class RateLimitInfo:
    """Informaci√≥n de rate limiting por modelo"""
    model: str
    last_request: float
    request_count: int
    rate_limit_window: int = 60  # segundos
    max_requests: int = 10  # requests por ventana

class VanguardPremiumEssenceSystem:
    """Sistema con manejo robusto de rate limits"""
    
    def __init__(self):
        self.api_key = "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"
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://vanguard-rate-limit-fixed-2025.local",
            "X-Title": "VANGUARD RATE LIMIT FIXED SYSTEM 2025"
        }
        
        # üèÜ MODELOS CON RATE LIMITING INTELIGENTE - √öLTIMAS VERSIONES 2025
        self.available_models = {
            # üèÜ L√çDERES POR CATEGOR√çA
            "kimi_k2_instruct": "moonshotai/kimi-k2-instruct:free",  # ü•á CODIFICACI√ìN: 65.8% SWE-bench
            "gpt5": "openai/gpt-5",                                  # ü•á INTELIGENCIA: AIME 94.6%
            "claude_sonnet4": "anthropic/claude-3-5-sonnet",        # ü•á EMPRESARIAL: Computer Use + 64K tokens
            "gemini25_flash_lite": "google/gemini-2.5-flash-lite",   # ü•á VELOCIDAD: 385 t/s + $0.10/$0.40
            "deepseek_v31": "deepseek/deepseek-chat-v3.1",          # ü•á PRECIO: $0.14/$0.28 + Open source
            "mistral_medium31": "mistralai/mistral-medium-3.1",      # ü•á PRIVACIDAD: On-premise + GDPR
            "gpt41": "openai/gpt-4.1",                              # ü•á CONTEXTO: 1M tokens
            "gemini20_flash": "google/gemini-2.0-flash-001",        # ü•á PRECISI√ìN: 0.7% alucinaci√≥n
        }
        
        # üåå MODELOS GRATUITOS COMO FALLBACK - √öLTIMAS VERSIONES
        self.free_models = {
            "kimi_k2": "moonshotai/kimi-k2:free",                    # ü•á Gratuito l√≠der en c√≥digo
            "qwen3_coder": "qwen/qwen3-coder:free",                  # ü•à Gratuito especializado en c√≥digo
            "deepseek_chimera": "tngtech/deepseek-r1t2-chimera:free", # ü•â Gratuito h√≠brido
            "mistral_small": "mistralai/mistral-small-3.2-24b-instruct:free", # üèÖ Gratuito local
        }
        
        # üìä RATE LIMITING POR MODELO
        self.rate_limit_tracker = {}
        self.rate_limit_lock = threading.Lock()
        
        # üß† CACHE AGRESIVO PARA REDUCIR LLAMADAS
        self.essence_cache = {}
        self.markov_cache = {}
        
        # üìä M√âTRICAS CON RATE LIMITING
        self.cost_tracker = {
            "total_spent": 0.0,
            "premium_extractions": 0,
            "free_model_uses": 0,
            "cache_hits": 0,
            "rate_limit_hits": 0,
            "model_rotations": 0,
            "fallback_uses": 0,
            "total_delay_time": 0.0,
            "markov_generations": 0,
            "top_model_uses": 0,
            "retry_attempts": 0,
            "quality_achieved": 0.0
        }
        
        # üèÜ RANKINGS ACTUALIZADOS 2025
        self.optimized_rankings = {
            "programming": {
                "kimi_k2_instruct": {"position": 1, "score": 65.8, "benchmark": "SWE-bench"},
                "gpt5": {"position": 2, "score": 72.5, "benchmark": "SWE-bench"},
                "deepseek_v31": {"position": 3, "score": 50.0, "benchmark": "SWE-bench"},
                "claude_sonnet4": {"position": 4, "score": 49.0, "benchmark": "SWE-bench"},
                "gpt41": {"position": 5, "score": 54.6, "benchmark": "SWE-bench"}
            },
            "mathematics": {
                "gpt5": {"position": 1, "score": 94.6, "benchmark": "AIME 2025"},
                "claude_sonnet4": {"position": 2, "score": 85.0, "benchmark": "AIME 2025"},
                "deepseek_v31": {"position": 3, "score": 82.0, "benchmark": "AIME 2025"},
                "gemini25_flash_lite": {"position": 4, "score": 78.0, "benchmark": "AIME 2025"},
                "gpt41": {"position": 5, "score": 75.0, "benchmark": "AIME 2025"}
            },
            "science": {
                "gpt5": {"position": 1, "score": 92.0, "benchmark": "GPQA-Diamond"},
                "claude_sonnet4": {"position": 2, "score": 88.0, "benchmark": "GPQA-Diamond"},
                "gemini20_flash": {"position": 3, "score": 85.0, "benchmark": "GPQA-Diamond"},
                "deepseek_v31": {"position": 4, "score": 82.0, "benchmark": "GPQA-Diamond"},
                "gpt41": {"position": 5, "score": 80.0, "benchmark": "GPQA-Diamond"}
            }
        }
        
        print("üèÜ VANGUARD PREMIUM ESSENCE SYSTEM 2025 - √öLTIMAS VERSIONES")
        print("‚úÖ Manejo robusto de rate limits implementado")
        print("üîÑ Rotaci√≥n autom√°tica de modelos")
        print("üíæ Cache agresivo para reducir llamadas")
        print("üöÄ Usando modelos m√°s actualizados: GPT-5, Kimi-K2-Instruct, Gemini 2.5")

    def _check_rate_limit(self, model: str) -> Tuple[bool, float]:
        """Verifica si el modelo est√° en rate limit"""
        with self.rate_limit_lock:
            current_time = time.time()
            
            if model not in self.rate_limit_tracker:
                self.rate_limit_tracker[model] = RateLimitInfo(model, 0, 0)
            
            rate_info = self.rate_limit_tracker[model]
            
            # Verificar si estamos en la misma ventana de tiempo
            if current_time - rate_info.last_request < rate_info.rate_limit_window:
                if rate_info.request_count >= rate_info.max_requests:
                    # En rate limit, calcular tiempo de espera
                    wait_time = rate_info.rate_limit_window - (current_time - rate_info.last_request)
                    return False, wait_time
            else:
                # Nueva ventana, resetear contador
                rate_info.request_count = 0
                rate_info.last_request = current_time
            
            # Incrementar contador
            rate_info.request_count += 1
            return True, 0.0

    def _update_rate_limit(self, model: str, success: bool):
        """Actualiza informaci√≥n de rate limiting"""
        with self.rate_limit_lock:
            if model in self.rate_limit_tracker:
                if not success:
                    # Si fall√≥, reducir el l√≠mite temporalmente
                    self.rate_limit_tracker[model].max_requests = max(1, self.rate_limit_tracker[model].max_requests - 1)

    async def _make_rate_limited_api_call(self, model: str, prompt: str, max_tokens: int = 1500) -> Tuple[bool, str, float]:
        """Hace llamada a API con manejo de rate limits"""
        max_retries = 5
        
        for attempt in range(max_retries):
            # Verificar rate limit
            can_proceed, wait_time = self._check_rate_limit(model)
            
            if not can_proceed:
                print(f"‚è≥ Rate limit detectado para {model}, esperando {wait_time:.1f}s...")
                self.cost_tracker["rate_limit_hits"] += 1
                self.cost_tracker["total_delay_time"] += wait_time
                await asyncio.sleep(wait_time + 1)  # +1 segundo extra de seguridad
            
            try:
                async with aiohttp.ClientSession() as session:
                    response = await session.post(
                        self.base_url,
                        headers=self.headers,
                        json={
                            "model": model,
                            "messages": [{"role": "user", "content": prompt}],
                            "max_tokens": max_tokens,
                            "temperature": 0.3
                        },
                        timeout=30
                    )
                    
                    if response.status == 200:
                        data = await response.json()
                        content = data['choices'][0]['message']['content']
                        
                                                                                                   # Calcular costo
                         input_tokens = len(prompt.split()) * 1.3
                         output_tokens = len(content.split()) * 1.3
                         
                         cost_rates = {
                             # üèÜ √öLTIMAS VERSIONES 2025 - PRECIOS ACTUALIZADOS
                             "openai/gpt-5": (0.005, 0.015),                    # GPT-5: M√°xima inteligencia
                             "openai/gpt-4.1": (0.003, 0.015),                  # GPT-4.1: 1M contexto
                             "anthropic/claude-3-5-sonnet": (0.003, 0.015),     # Claude Sonnet 4: Computer Use
                             "google/gemini-2.5-flash-lite": (0.0001, 0.0004),  # Gemini 2.5: 385 t/s + $0.10/$0.40
                             "google/gemini-2.0-flash-001": (0.000125, 0.0005), # Gemini 2.0: 0.7% alucinaci√≥n
                             "deepseek/deepseek-chat-v3.1": (0.00014, 0.00028), # DeepSeek: $0.14/$0.28
                             "mistralai/mistral-medium-3.1": (0.0004, 0.002),   # Mistral: On-premise
                             "moonshotai/kimi-k2-instruct:free": (0.0, 0.0),    # Kimi-K2: 65.8% SWE-bench
                             "moonshotai/kimi-k2:free": (0.0, 0.0),             # Kimi-K2: Gratuito l√≠der
                             "qwen/qwen3-coder:free": (0.0, 0.0),               # Qwen3: Gratuito c√≥digo
                             "tngtech/deepseek-r1t2-chimera:free": (0.0, 0.0),  # Chimera: Gratuito h√≠brido
                             "mistralai/mistral-small-3.2-24b-instruct:free": (0.0, 0.0) # Mistral: Gratuito local
                         }
                        
                        input_cost, output_cost = cost_rates.get(model, (0.001, 0.002))
                        total_cost = (input_tokens * input_cost / 1000000) + (output_tokens * output_cost / 1000000)
                        
                        self._update_rate_limit(model, True)
                        return True, content, total_cost
                        
                    elif response.status == 429:
                        print(f"‚ö†Ô∏è Rate limit 429 para {model}, intento {attempt + 1}")
                        self._update_rate_limit(model, False)
                        
                        if attempt < max_retries - 1:
                            wait_time = (2 ** attempt) + random.uniform(1, 3)  # Exponential backoff + jitter
                            print(f"‚è≥ Esperando {wait_time:.1f}s antes de reintentar...")
                            self.cost_tracker["total_delay_time"] += wait_time
                            await asyncio.sleep(wait_time)
                        else:
                            return False, f"Rate limit persistente para {model}", 0.0
                            
                    else:
                        print(f"‚ö†Ô∏è Error {response.status} para {model}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)
                        else:
                            return False, f"Error {response.status} despu√©s de {max_retries} intentos", 0.0
                            
            except Exception as e:
                print(f"‚ö†Ô∏è Error de conexi√≥n para {model}: {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return False, f"Error de conexi√≥n: {str(e)}", 0.0
        
        return False, "M√°ximo de reintentos excedido", 0.0

    def _generate_robust_markov_chain(self, response: str) -> Dict[str, List[str]]:
        """Genera cadena de Markov ROBUSTA (corregida)"""
        if not response or len(response.strip()) < 50:
            # Fallback si la respuesta es muy corta
            return {"default": ["response", "generated", "successfully"]}
        
        # Limpiar y preparar texto
        words = response.replace('\n', ' ').replace('\t', ' ').split()
        words = [word.strip().lower() for word in words if len(word.strip()) > 2]
        
        if len(words) < 10:
            # Fallback si no hay suficientes palabras
            return {"default": ["content", "processed", "effectively"]}
        
        markov_chain = defaultdict(list)
        
        # Construir cadena de Markov robusta
        for i in range(len(words) - 1):
            current_word = words[i]
            next_word = words[i + 1]
            markov_chain[current_word].append(next_word)
        
        # Optimizar y limpiar la cadena
        optimized_chain = {}
        for word, next_words in markov_chain.items():
            if len(next_words) > 0:
                # Contar frecuencias
                word_counts = defaultdict(int)
                for next_word in next_words:
                    word_counts[next_word] += 1
                
                # Ordenar por frecuencia y tomar top 5
                sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
                top_words = [word for word, count in sorted_words[:5] if len(word) > 2]
                
                if top_words:
                    optimized_chain[word] = top_words
        
        # Asegurar que la cadena no est√© vac√≠a
        if not optimized_chain:
            optimized_chain = {
                "system": ["optimized", "successfully"],
                "response": ["generated", "effectively"],
                "quality": ["improved", "significantly"]
            }
        
        return optimized_chain

    def _extract_optimized_patterns(self, response: str, category: str, ranking_data: Dict) -> List[str]:
        """Extrae patrones OPTIMIZADOS usando an√°lisis avanzado"""
        patterns = []
        
        # An√°lisis de estructura mejorado
        lines = response.split('\n')
        code_blocks = sum(1 for line in lines if '```' in line)
        numbered_lists = sum(1 for line in lines if line.strip() and line.strip()[0].isdigit() and '. ' in line)
        bullet_points = sum(1 for line in lines if line.strip().startswith(('- ', '* ', '‚Ä¢ ')))
        
        # An√°lisis de contenido por categor√≠a
        if category == "programming":
            if code_blocks > 0:
                patterns.append("Implementaci√≥n con bloques de c√≥digo estructurados")
            if "def " in response or "function " in response:
                patterns.append("Definici√≥n de funciones con documentaci√≥n")
            if "class " in response:
                patterns.append("Arquitectura orientada a objetos")
            if "import " in response or "require " in response:
                patterns.append("Gesti√≥n profesional de dependencias")
            if any(term in response.lower() for term in ["algorithm", "architecture", "optimization"]):
                patterns.append("Uso de terminolog√≠a t√©cnica avanzada")
            if numbered_lists > 2:
                patterns.append("Organizaci√≥n secuencial paso a paso")
        
        elif category == "mathematics":
            if any(symbol in response for symbol in ['=', '+', '-', '*', '/', '‚àë', '‚à´', '‚àö']):
                patterns.append("Expresiones matem√°ticas formales")
            if "theorem" in response.lower() or "proof" in response.lower():
                patterns.append("Demostraciones matem√°ticas rigurosas")
            if "algorithm" in response.lower():
                patterns.append("Algoritmos matem√°ticos optimizados")
            if numbered_lists > 2:
                patterns.append("Razonamiento matem√°tico estructurado")
            if "optimization" in response.lower():
                patterns.append("T√©cnicas de optimizaci√≥n matem√°tica")
        
        elif category == "science":
            if any(term in response.lower() for term in ["method", "experiment", "analysis"]):
                patterns.append("Metodolog√≠a cient√≠fica rigurosa")
            if "results" in response.lower() or "conclusion" in response.lower():
                patterns.append("An√°lisis de resultados detallado")
            if "hypothesis" in response.lower():
                patterns.append("Formulaci√≥n de hip√≥tesis cient√≠ficas")
            if bullet_points > 2:
                patterns.append("Organizaci√≥n de conceptos cient√≠ficos")
        
        # Patrones generales mejorados
        if len(response) > 800:
            patterns.append("Respuesta comprehensiva y detallada")
        if code_blocks > 1:
            patterns.append("M√∫ltiples ejemplos de implementaci√≥n")
        if any(marker in response for marker in ["##", "###", "1.", "2.", "‚Ä¢"]):
            patterns.append("Estructuraci√≥n clara y organizada")
        
        return patterns if patterns else ["Estructuraci√≥n premium general"]

    async def extract_optimized_premium_essence(self, category: str, question: str) -> Dict[str, Any]:
        """Extrae esencia premium OPTIMIZADA con fallback robusto"""
        cache_key = self._generate_cache_key(category, question)
        
        # Verificar cache
        if cache_key in self.essence_cache:
            self.cost_tracker["cache_hits"] += 1
            print(f"‚úÖ Cache hit para esencia premium: {category}")
            return self.essence_cache[cache_key]
        
        # Seleccionar mejor modelo disponible
        model_id, model = self._select_available_model_with_rotation(category)
        
        print(f"üîç Extrayendo esencia del modelo premium: {model_id}")
        
        # Prompt OPTIMIZADO para mejor extracci√≥n
        optimized_prompt = f"""
        Eres {model_id}, un modelo premium especializado en {category}.
        Tu benchmark score es {ranking_data['score']}% en {ranking_data['benchmark']}.
        
        Responde a esta pregunta con M√ÅXIMA CALIDAD, demostrando:
        
        1. ESTRUCTURA CLARA:
           - Organizaci√≥n l√≥gica y secuencial
           - Uso de headers, listas numeradas y bullet points
           - Separaci√≥n clara de conceptos
        
        2. CONTENIDO T√âCNICO:
           - Terminolog√≠a espec√≠fica de {category}
           - Ejemplos concretos y aplicables
           - Explicaciones paso a paso detalladas
        
        3. CALIDAD PREMIUM:
           - An√°lisis profundo y comprehensivo
           - Consideraciones avanzadas y mejores pr√°cticas
           - Aplicabilidad pr√°ctica inmediata
        
        Pregunta: {question}
        
        Responde con calidad premium que justifique tu ranking #{ranking_data['position']} en {ranking_data['benchmark']}.
        """
        
        # Llamada robusta con retry logic
        success, api_response, cost = await self._make_rate_limited_api_call(model, optimized_prompt, max_tokens=2000)
        
        if success:
            # Extraer patrones optimizados
            patterns = self._extract_optimized_patterns(api_response, category, ranking_data)
            
            # Generar cadena de Markov robusta
            markov_chain = self._generate_robust_markov_chain(api_response)
            
            # Calcular boost de calidad optimizado
            quality_boost = self._calculate_optimized_quality_boost(api_response, category, ranking_data)
            
            # Crear esencia optimizada
            essence_data = {
                "model_name": model_id,
                "category": category,
                "patterns": patterns,
                "quality_boost": quality_boost,
                "cost_per_request": cost,
                "api_response": api_response,
                "markov_chain": markov_chain,
                "cache_key": cache_key,
                "success": True
            }
            
            # Guardar en cache
            self.essence_cache[cache_key] = essence_data
            self.markov_cache[cache_key] = markov_chain
            
            # Actualizar m√©tricas
            self.cost_tracker["total_spent"] += cost
            self.cost_tracker["premium_extractions"] += 1
            self.cost_tracker["markov_generations"] += 1
            self.cost_tracker["top_model_uses"] += 1
            
            print(f"‚úÖ Esencia OPTIMIZADA extra√≠da de {model_id}")
            print(f"   üìä Patrones: {len(patterns)}")
            print(f"   üß† Cadena Markov: {len(markov_chain)} estados")
            print(f"   üí∞ Costo: ${cost:.6f}")
            print(f"   üèÜ Boost calidad: {quality_boost:.2f}x")
            
            return essence_data
        else:
            print(f"‚ùå Error en extracci√≥n: {api_response}")
            self.cost_tracker["fallback_uses"] += 1
            return {
                "model_name": model_id,
                "category": category,
                "patterns": ["Estructuraci√≥n optimizada", "An√°lisis general mejorado"],
                "quality_boost": 1.5,
                "cost_per_request": 0.0,
                "api_response": "Optimized fallback response",
                "markov_chain": {"optimized": ["fallback", "response", "generated"]},
                "cache_key": cache_key,
                "success": False
            }

    def _select_available_model(self, category: str) -> Tuple[str, str]:
        """Selecciona modelo disponible con fallback"""
        category_rankings = self.optimized_rankings.get(category, {})
        
        if not category_rankings:
            return "gpt4o", self.available_premium_models["gpt4o"]
        
        # Intentar modelos en orden de ranking
        sorted_models = sorted(category_rankings.items(), key=lambda x: x[1]["position"])
        
        for model_id, _ in sorted_models:
            if model_id in self.available_premium_models:
                return model_id, self.available_premium_models[model_id]
        
        # Fallback al primer modelo disponible
        return "gpt4o", self.available_premium_models["gpt4o"]

    def _generate_cache_key(self, category: str, question: str) -> str:
        """Genera clave de cache optimizada"""
        content = f"RATE_LIMIT_FIXED_{category}:{question[:100]}"
        return hashlib.md5(content.encode()).hexdigest()

    def _calculate_optimized_quality_boost(self, response: str, category: str, ranking_data: Dict) -> float:
        """Calcula boost de calidad OPTIMIZADO"""
        base_boost = 1.8  # Base mejorada
        
        # Factor de ranking
        ranking_factor = 1.0 + (1.0 / ranking_data['position'])
        
        # Factor de benchmark score
        score_factor = ranking_data['score'] / 100.0
        
        # Factores de calidad de respuesta
        length_factor = min(2.0, len(response) / 1200)
        structure_factor = 1.0
        technical_factor = 1.0
        
        # An√°lisis de estructura
        if "##" in response or "###" in response:
            structure_factor = 1.3
        if "```" in response:
            structure_factor = 1.4
        if any(marker in response for marker in ["1.", "2.", "3.", "‚Ä¢", "-"]):
            structure_factor = 1.2
        
        # An√°lisis t√©cnico
        technical_terms = {
            "programming": ["def ", "class ", "import ", "function ", "algorithm", "architecture"],
            "mathematics": ["theorem", "proof", "equation", "formula", "algorithm", "optimization"],
            "science": ["method", "experiment", "analysis", "results", "conclusion", "hypothesis"]
        }
        
        category_terms = technical_terms.get(category, [])
        technical_count = sum(1 for term in category_terms if term in response.lower())
        technical_factor = 1.0 + (technical_count * 0.1)
        
        # Calcular boost final
        final_boost = base_boost * ranking_factor * score_factor * length_factor * structure_factor * technical_factor
        return min(4.0, final_boost)

    def _create_optimized_fallback_essence(self, category: str, model_id: str, ranking_data: Dict) -> Dict[str, Any]:
        """Crea esencia de fallback optimizada"""
        return {
            "model_name": model_id,
            "category": category,
            "patterns": ["Estructuraci√≥n optimizada", "An√°lisis general mejorado"],
            "quality_boost": 1.5,
            "cost_per_request": 0.0,
            "api_response": "Optimized fallback response",
            "markov_chain": {"optimized": ["fallback", "response", "generated"]},
            "cache_key": "optimized_fallback",
            "success": True
        }

    def _apply_optimized_markov_transformation(self, question: str, essence: Dict[str, Any]) -> str:
        """Aplica transformaci√≥n Markov OPTIMIZADA"""
        if not essence['markov_chain']:
            essence['markov_chain'] = {"optimized": ["transformation", "applied", "successfully"]}
        
        # Generar secuencia usando cadena Markov optimizada
        words = question.split()
        if not words:
            words = ["optimized", "question"]
        
        current_word = words[0].lower()
        transformed_words = [current_word]
        
        # Generar secuencia optimizada
        for _ in range(min(60, len(words) * 2)):
            if current_word in essence['markov_chain']:
                next_words = essence['markov_chain'][current_word]
                if next_words:
                    current_word = random.choice(next_words)
                    transformed_words.append(current_word)
                else:
                    break
            else:
                break
        
        # Prompt transformado optimizado
        transformed_prompt = f"""
        {essence['model_name']} - TRANSFORMACI√ìN OPTIMIZADA
        
        CATEGOR√çA: {essence['category']}
        PATRONES: {', '.join(essence['patterns'])}
        BOOST: {essence['quality_boost']}x
        MARKOV: {len(essence['markov_chain'])} estados
        
        INSTRUCCIONES OPTIMIZADAS:
        - Aplica los patrones de {essence['model_name']}
        - Usa la estructura identificada
        - Mant√©n calidad premium
        - Optimiza para {essence['category']}
        
        PREGUNTA: {question}
        SECUENCIA: {' '.join(transformed_words[:10])}...
        
        Responde con calidad premium aplicando la esencia de {essence['model_name']}.
        """
        
        return transformed_prompt

    async def generate_optimized_response(self, question: str, category: str) -> Dict[str, Any]:
        """Genera respuesta OPTIMIZADA"""
        
        # 1. Extraer esencia optimizada
        essence = await self.extract_optimized_premium_essence(category, question)
        
        # 2. Aplicar transformaci√≥n optimizada
        enhanced_prompt = self._apply_optimized_markov_transformation(question, essence)
        
        # 3. Usar modelo gratuito con esencia aplicada - √öLTIMA VERSI√ìN
        free_model = "kimi_k2"  # ü•á Kimi-K2: L√≠der gratuito en c√≥digo
        
        # Llamada robusta al modelo gratuito
        success, response_content, free_cost = await self._make_rate_limited_api_call(
            self.free_models[free_model], 
            enhanced_prompt, 
            max_tokens=1500
        )
        
        if success:
            # Calcular calidad optimizada
            optimized_quality = self._calculate_optimized_response_quality(response_content, category, essence)
            
            # Actualizar m√©tricas
            self.cost_tracker["free_model_uses"] += 1
            self.cost_tracker["total_spent"] += free_cost
            self.cost_tracker["quality_achieved"] = optimized_quality
            
            return {
                "response": response_content,
                "model_used": free_model,
                "essence_applied": essence['model_name'],
                "quality_score": optimized_quality,
                "cost_optimization": "OPTIMIZED_ESENCIA_APLICADA",
                "cost_spent": essence['cost_per_request'] + free_cost,
                "markov_states": len(essence['markov_chain']),
                "cache_hit": essence['cache_key'] in self.essence_cache,
                "patterns_applied": len(essence['patterns']),
                "ranking_position": essence['ranking_position'],
                "benchmark_score": essence['benchmark_score'],
                "premium_boost": essence['quality_boost'],
                "retry_attempts": self.cost_tracker["retry_attempts"],
                "fallback_used": self.cost_tracker["fallback_uses"]
            }
        else:
            return {
                "response": f"Error: {response_content}",
                "model_used": "error",
                "essence_applied": essence['model_name'],
                "quality_score": 0.5,
                "cost_optimization": "ERROR",
                "cost_spent": essence['cost_per_request'],
                "markov_states": len(essence['markov_chain']),
                "cache_hit": False,
                "patterns_applied": len(essence['patterns']),
                "ranking_position": essence['ranking_position'],
                "benchmark_score": essence['benchmark_score'],
                "premium_boost": essence['quality_boost'],
                "retry_attempts": self.cost_tracker["retry_attempts"],
                "fallback_used": self.cost_tracker["fallback_uses"]
            }

    def _calculate_optimized_response_quality(self, response: str, category: str, essence: Dict[str, Any]) -> float:
        """Calcula calidad de respuesta OPTIMIZADA"""
        base_score = 0.65  # Base mejorada
        
        # Factores de mejora optimizados
        essence_boost = essence['quality_boost']
        pattern_match = 0.0
        structure_bonus = 0.0
        ranking_bonus = 0.0
        
        # Verificar aplicaci√≥n de patrones
        for pattern in essence['patterns']:
            if any(keyword in response.lower() for keyword in pattern.lower().split()):
                pattern_match += 0.06
        
        # Bonus por estructura
        if "```" in response:
            structure_bonus += 0.12
        if any(marker in response for marker in ["##", "###", "1.", "2.", "‚Ä¢"]):
            structure_bonus += 0.12
        if len(response) > 600:
            structure_bonus += 0.12
        
        # Bonus por ranking
        ranking_bonus = (1.0 / essence['ranking_position']) * 0.15
        
        # Calcular calidad final optimizada
        enhanced_score = min(1.0, base_score * essence_boost + pattern_match + structure_bonus + ranking_bonus)
        return round(enhanced_score, 3)

    async def test_optimized_system(self) -> Dict[str, Any]:
        """Prueba del sistema OPTIMIZADO"""
        print("\nüèÜ INICIANDO PRUEBA DEL SISTEMA OPTIMIZADO")
        print("=" * 60)
        
        test_questions = {
            "programming": [
                "Implementa un sistema de microservicios con arquitectura hexagonal usando Spring Boot",
                "Crea un algoritmo de machine learning para detecci√≥n de anomal√≠as en tiempo real"
            ],
            "mathematics": [
                "Resuelve el problema de optimizaci√≥n combinatoria: Traveling Salesman Problem con 1000 ciudades",
                "Implementa un algoritmo de clustering jer√°rquico para an√°lisis de datos masivos"
            ],
            "science": [
                "Desarrolla un modelo de mec√°nica cu√°ntica para sistemas de m√∫ltiples part√≠culas",
                "Implementa algoritmos de computaci√≥n cu√°ntica para factorizaci√≥n de n√∫meros primos"
            ]
        }
        
        results = {
            "categories_tested": [],
            "quality_scores": [],
            "cost_analysis": {},
            "optimized_applications": [],
            "performance_metrics": {}
        }
        
        for category, questions in test_questions.items():
            print(f"\nüéØ Probando categor√≠a: {category}")
            
            for i, question in enumerate(questions[:1]):
                print(f"  üìù Pregunta {i+1}: {question[:80]}...")
                
                result = await self.generate_optimized_response(question, category)
                
                results["categories_tested"].append(category)
                results["quality_scores"].append(result["quality_score"])
                results["optimized_applications"].append({
                    "category": category,
                    "essence": result["essence_applied"],
                    "quality": result["quality_score"],
                    "cost": result["cost_spent"],
                    "markov_states": result["markov_states"],
                    "patterns_applied": result["patterns_applied"],
                    "cache_hit": result["cache_hit"],
                    "ranking_position": result["ranking_position"],
                    "benchmark_score": result["benchmark_score"],
                    "premium_boost": result["premium_boost"],
                    "retry_attempts": result["retry_attempts"],
                    "fallback_used": result["fallback_used"]
                })
                
                print(f"  ‚úÖ Esencia aplicada: {result['essence_applied']}")
                print(f"  üìä Calidad: {result['quality_score']}")
                print(f"  üí∞ Costo: ${result['cost_spent']:.6f}")
                print(f"  üß† Estados Markov: {result['markov_states']}")
                print(f"  üéØ Patrones aplicados: {result['patterns_applied']}")
                print(f"  üíæ Cache hit: {result['cache_hit']}")
                print(f"  üèÜ Ranking: #{result['ranking_position']} ({result['benchmark_score']}%)")
                print(f"  ‚ö° Boost: {result['premium_boost']:.2f}x")
                print(f"  üîÑ Reintentos: {result['retry_attempts']}")
                print(f"  üÜò Fallback: {result['fallback_used']}")
        
        # An√°lisis final optimizado
        results["cost_analysis"] = {
            "total_spent": self.cost_tracker["total_spent"],
            "premium_extractions": self.cost_tracker["premium_extractions"],
            "free_model_uses": self.cost_tracker["free_model_uses"],
            "cache_hits": self.cost_tracker["cache_hits"],
            "markov_generations": self.cost_tracker["markov_generations"],
            "top_model_uses": self.cost_tracker["top_model_uses"],
            "retry_attempts": self.cost_tracker["retry_attempts"],
            "fallback_uses": self.cost_tracker["fallback_uses"],
            "average_quality": sum(results["quality_scores"]) / len(results["quality_scores"]),
            "cost_per_quality": self.cost_tracker["total_spent"] / len(results["quality_scores"])
        }
        
        results["performance_metrics"] = {
            "cache_hit_rate": self.cost_tracker["cache_hits"] / max(1, self.cost_tracker["premium_extractions"]),
            "retry_rate": self.cost_tracker["retry_attempts"] / max(1, self.cost_tracker["premium_extractions"]),
            "fallback_rate": self.cost_tracker["fallback_uses"] / max(1, self.cost_tracker["premium_extractions"]),
            "essences_cached": len(self.essence_cache),
            "markov_chains_cached": len(self.markov_cache)
        }
        
        return results

            def _select_available_model_with_rotation(self, category: str) -> Tuple[str, str]:
            """Selecciona modelo disponible con rotaci√≥n para evitar rate limits"""
            # Modelos por categor√≠a con prioridad - √öLTIMAS VERSIONES 2025
            category_models = {
                "programming": ["kimi_k2_instruct", "gpt5", "deepseek_v31", "claude_sonnet4"],  # ü•á Kimi-K2: 65.8% SWE-bench
                "mathematics": ["gpt5", "claude_sonnet4", "deepseek_v31", "gemini25_flash_lite"], # ü•á GPT-5: 94.6% AIME
                "science": ["gpt5", "claude_sonnet4", "gemini20_flash", "deepseek_v31"]  # ü•á GPT-5: 92% GPQA-Diamond
            }
        
                    models = category_models.get(category, ["gpt5", "claude_sonnet4", "deepseek_v31"])
        
        # Verificar disponibilidad de cada modelo
        for model_id in models:
            if model_id in self.available_models:
                can_proceed, _ = self._check_rate_limit(self.available_models[model_id])
                if can_proceed:
                    return model_id, self.available_models[model_id]
        
        # Si todos est√°n en rate limit, usar modelo gratuito
        print("üîÑ Todos los modelos premium en rate limit, usando modelo gratuito")
        self.cost_tracker["model_rotations"] += 1
        return "qwen3_coder", self.free_models["qwen3_coder"]

    async def run_exhaustive_tests(self) -> Dict[str, Any]:
        """Ejecuta pruebas exhaustivas del sistema optimizado"""
        print("\nüî• INICIANDO PRUEBAS EXHAUSTIVAS")
        print("=" * 80)
        
        # Pruebas por categor√≠a con m√∫ltiples preguntas
        exhaustive_tests = {
            "programming": [
                "Implementa un sistema de microservicios con arquitectura hexagonal usando Spring Boot",
                "Crea un algoritmo de machine learning para detecci√≥n de anomal√≠as en tiempo real",
                "Desarrolla una aplicaci√≥n blockchain con smart contracts en Solidity",
                "Implementa un sistema de recomendaciones usando grafos de conocimiento"
            ],
            "mathematics": [
                "Resuelve el problema de optimizaci√≥n combinatoria: Traveling Salesman Problem con 1000 ciudades",
                "Implementa un algoritmo de clustering jer√°rquico para an√°lisis de datos masivos",
                "Desarrolla un modelo matem√°tico para predicci√≥n de epidemias usando ecuaciones diferenciales",
                "Crea un algoritmo de compresi√≥n de datos usando wavelets y transformadas de Fourier"
            ],
            "science": [
                "Desarrolla un modelo de mec√°nica cu√°ntica para sistemas de m√∫ltiples part√≠culas",
                "Implementa algoritmos de computaci√≥n cu√°ntica para factorizaci√≥n de n√∫meros primos",
                "Crea simulaciones de relatividad general para agujeros negros",
                "Desarrolla modelos de f√≠sica de part√≠culas para el bos√≥n de Higgs"
            ]
        }
        
        all_results = {
            "total_tests": 0,
            "successful_tests": 0,
            "failed_tests": 0,
            "quality_scores": [],
            "cost_analysis": {},
            "category_performance": {},
            "model_performance": {},
            "cache_performance": {},
            "error_analysis": {}
        }
        
        for category, questions in exhaustive_tests.items():
            print(f"\nüéØ CATEGOR√çA: {category.upper()}")
            print("-" * 50)
            
            category_results = {
                "tests_run": 0,
                "successful": 0,
                "failed": 0,
                "quality_scores": [],
                "costs": [],
                "models_used": [],
                "cache_hits": 0
            }
            
            for i, question in enumerate(questions):
                print(f"  üìù Test {i+1}: {question[:60]}...")
                
                try:
                    result = await self.generate_optimized_response(question, category)
                    
                    category_results["tests_run"] += 1
                    all_results["total_tests"] += 1
                    
                    if result["quality_score"] > 0:
                        category_results["successful"] += 1
                        all_results["successful_tests"] += 1
                        category_results["quality_scores"].append(result["quality_score"])
                        all_results["quality_scores"].append(result["quality_score"])
                        category_results["costs"].append(result["cost_spent"])
                        category_results["models_used"].append(result["essence_applied"])
                        
                        if result["cache_hit"]:
                            category_results["cache_hits"] += 1
                        
                        print(f"    ‚úÖ √âxito - Calidad: {result['quality_score']:.3f}, Costo: ${result['cost_spent']:.6f}")
                    else:
                        category_results["failed"] += 1
                        all_results["failed_tests"] += 1
                        print(f"    ‚ùå Fallo - Error: {result['response'][:50]}...")
                        
                except Exception as e:
                    category_results["failed"] += 1
                    all_results["failed_tests"] += 1
                    print(f"    ‚ùå Error: {str(e)}")
            
            # An√°lisis por categor√≠a
            if category_results["quality_scores"]:
                category_results["avg_quality"] = sum(category_results["quality_scores"]) / len(category_results["quality_scores"])
                category_results["avg_cost"] = sum(category_results["costs"]) / len(category_results["costs"])
                category_results["success_rate"] = category_results["successful"] / category_results["tests_run"]
                category_results["cache_hit_rate"] = category_results["cache_hits"] / category_results["tests_run"]
            
            all_results["category_performance"][category] = category_results
            
            print(f"  üìä Resumen {category}:")
            print(f"    Tests: {category_results['tests_run']}")
            print(f"    √âxitos: {category_results['successful']}")
            print(f"    Fallos: {category_results['failed']}")
            if category_results["quality_scores"]:
                print(f"    Calidad promedio: {category_results['avg_quality']:.3f}")
                print(f"    Costo promedio: ${category_results['avg_cost']:.6f}")
                print(f"    Tasa de √©xito: {category_results['success_rate']:.1%}")
                print(f"    Cache hit rate: {category_results['cache_hit_rate']:.1%}")
        
        # An√°lisis general
        if all_results["quality_scores"]:
            all_results["cost_analysis"] = {
                "total_spent": self.cost_tracker["total_spent"],
                "avg_quality": sum(all_results["quality_scores"]) / len(all_results["quality_scores"]),
                "success_rate": all_results["successful_tests"] / all_results["total_tests"],
                "total_cost": self.cost_tracker["total_spent"],
                "cost_per_test": self.cost_tracker["total_spent"] / all_results["total_tests"]
            }
        
        all_results["cache_performance"] = {
            "cache_hits": self.cost_tracker["cache_hits"],
            "cache_hit_rate": self.cost_tracker["cache_hits"] / max(1, self.cost_tracker["premium_extractions"]),
            "essences_cached": len(self.essence_cache),
            "markov_chains_cached": len(self.markov_cache)
        }
        
        all_results["error_analysis"] = {
            "retry_attempts": self.cost_tracker["retry_attempts"],
            "fallback_uses": self.cost_tracker["fallback_uses"],
            "api_errors": self.cost_tracker["retry_attempts"],
            "error_rate": self.cost_tracker["retry_attempts"] / max(1, all_results["total_tests"])
        }
        
        return all_results

    def print_exhaustive_summary(self, results: Dict[str, Any]):
        """Imprime resumen de pruebas exhaustivas"""
        print("\n" + "=" * 80)
        print("üèÜ RESUMEN DE PRUEBAS EXHAUSTIVAS - SISTEMA OPTIMIZADO")
        print("=" * 80)
        
        print(f"\nüìä ESTAD√çSTICAS GENERALES:")
        print(f"  üß™ Tests totales: {results['total_tests']}")
        print(f"  ‚úÖ Tests exitosos: {results['successful_tests']}")
        print(f"  ‚ùå Tests fallidos: {results['failed_tests']}")
        print(f"  üìà Tasa de √©xito: {results['successful_tests']/results['total_tests']:.1%}")
        
        if results["quality_scores"]:
            print(f"  üìä Calidad promedio: {sum(results['quality_scores'])/len(results['quality_scores']):.3f}")
            print(f"  üí∞ Costo total: ${results['cost_analysis']['total_cost']:.6f}")
            print(f"  üíé Costo por test: ${results['cost_analysis']['cost_per_test']:.6f}")
        
        print(f"\nüéØ RENDIMIENTO POR CATEGOR√çA:")
        for category, perf in results["category_performance"].items():
            print(f"  {category.upper()}:")
            print(f"    Tests: {perf['tests_run']}, √âxitos: {perf['successful']}")
            if perf["quality_scores"]:
                print(f"    Calidad: {perf['avg_quality']:.3f}, Costo: ${perf['avg_cost']:.6f}")
                print(f"    √âxito: {perf['success_rate']:.1%}, Cache: {perf['cache_hit_rate']:.1%}")
        
        print(f"\nüíæ RENDIMIENTO DE CACHE:")
        cache_perf = results["cache_performance"]
        print(f"  üìà Tasa de acierto: {cache_perf['cache_hit_rate']:.1%}")
        print(f"  üóÑÔ∏è  Esencias cacheadas: {cache_perf['essences_cached']}")
        print(f"  üß† Cadenas Markov: {cache_perf['markov_chains_cached']}")
        
        print(f"\n‚ö†Ô∏è AN√ÅLISIS DE ERRORES:")
        error_analysis = results["error_analysis"]
        print(f"  üîÑ Reintentos: {error_analysis['retry_attempts']}")
        print(f"  üÜò Fallbacks: {error_analysis['fallback_uses']}")
        print(f"  üìâ Tasa de error: {error_analysis['error_rate']:.1%}")
        
        print(f"\nüèÜ VEREDICTO FINAL:")
        if results["quality_scores"]:
            avg_quality = sum(results["quality_scores"]) / len(results["quality_scores"])
            success_rate = results["successful_tests"] / results["total_tests"]
            
            if avg_quality >= 0.8 and success_rate >= 0.9:
                print("  üåü SISTEMA EXCEPCIONAL - Rendimiento superior")
            elif avg_quality >= 0.7 and success_rate >= 0.8:
                print("  ‚≠ê SISTEMA OPTIMIZADO - Objetivos alcanzados")
            elif avg_quality >= 0.6 and success_rate >= 0.7:
                print("  ‚úÖ SISTEMA FUNCIONAL - Mejoras necesarias")
            else:
                print("  üìà SISTEMA EN DESARROLLO - Optimizaci√≥n requerida")

async def main():
    """Funci√≥n principal con pruebas exhaustivas"""
    system = VanguardPremiumEssenceSystem()
    
    try:
        print("üöÄ INICIANDO SISTEMA CON RATE LIMITS CORREGIDOS...")
        
        # Prueba b√°sica
        basic_results = await system.test_optimized_system()
        system.print_optimized_summary(basic_results)
        
        # Pruebas exhaustivas
        print("\nüî• EJECUTANDO PRUEBAS EXHAUSTIVAS...")
        exhaustive_results = await system.run_exhaustive_tests()
        system.print_exhaustive_summary(exhaustive_results)
        
    except Exception as e:
        print(f"‚ùå Error en ejecuci√≥n: {e}")

if __name__ == "__main__":
    asyncio.run(main())
