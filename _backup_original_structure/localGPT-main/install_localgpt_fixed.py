#!/usr/bin/env python3
"""
Script de instalaci√≥n mejorado para localGPT
Maneja errores comunes de instalaci√≥n en Windows
"""

import subprocess
import sys
import os
import platform

def run_command(command, check=True, capture_output=True):
    """Ejecuta un comando y muestra la salida"""
    print(f"\n>>> Ejecutando: {command}")
    try:
        result = subprocess.run(command, shell=True, check=check, 
                              capture_output=capture_output, text=True)
        if result.stdout:
            print(result.stdout)
        if result.stderr and result.returncode != 0:
            print("STDERR:", result.stderr)
        return result.returncode == 0
    except subprocess.CalledProcessError as e:
        print(f"Error ejecutando comando: {e}")
        if e.stderr:
            print(f"Error: {e.stderr}")
        return False

def check_python_version():
    """Verifica que Python sea 3.10 o superior"""
    version = sys.version_info
    print(f"Versi√≥n de Python detectada: {version.major}.{version.minor}.{version.micro}")
    
    if version.major < 3 or (version.major == 3 and version.minor < 10):
        print("‚ùå ERROR: Se requiere Python 3.10 o superior")
        print("Por favor instala Python 3.10+ desde https://python.org")
        return False
    
    print("‚úÖ Versi√≥n de Python compatible")
    return True

def check_gpu():
    """Detecta si hay GPU NVIDIA disponible"""
    try:
        result = subprocess.run("nvidia-smi", shell=True, capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ GPU NVIDIA detectada")
            print(result.stdout.split('\n')[0])  # Primera l√≠nea con info de la GPU
            return True
    except:
        pass
    
    print("‚ö†Ô∏è  No se detect√≥ GPU NVIDIA, se usar√° CPU")
    return False

def create_requirements_safe():
    """Crea un requirements.txt sin paquetes problem√°ticos para Windows"""
    safe_requirements = """# Natural Language Processing - Version segura para Windows
langchain==0.0.267
chromadb==0.4.6
pdfminer.six==20221105
InstructorEmbedding
sentence-transformers==2.2.2
faiss-cpu
huggingface_hub==0.25.0
transformers
protobuf==3.20.2
docx2txt
unstructured
# unstructured[pdf]  # Comentado para evitar errores

# Utilities
urllib3==1.26.6
accelerate
click
flask
requests

# Streamlit related
streamlit
Streamlit-extras

# Excel File Manipulation
openpyxl

# Windows specific
bitsandbytes-windows
"""
    
    with open("requirements_safe.txt", "w") as f:
        f.write(safe_requirements)
    print("‚úÖ Archivo requirements_safe.txt creado")

def install_core_packages():
    """Instala paquetes b√°sicos uno por uno"""
    print("\nüîß Instalando paquetes b√°sicos...")
    
    # Actualizar pip primero
    if not run_command(f"{sys.executable} -m pip install --upgrade pip"):
        print("‚ùå Error actualizando pip")
        return False
    
    # Paquetes esenciales en orden espec√≠fico
    core_packages = [
        "wheel",
        "setuptools",
        "numpy",
        "torch",
        "langchain==0.0.267",
        "transformers",
        "sentence-transformers==2.2.2",
        "chromadb==0.4.6",
        "faiss-cpu",
        "streamlit",
        "flask",
        "requests",
        "InstructorEmbedding",
        "accelerate",
        "huggingface_hub==0.25.0",
        "pdfminer.six==20221105",
        "docx2txt",
        "unstructured",
        "openpyxl",
        "protobuf==3.20.2",
        "urllib3==1.26.6",
        "click",
        "Streamlit-extras"
    ]
    
    failed_packages = []
    
    for package in core_packages:
        print(f"\nüì¶ Instalando {package}...")
        if not run_command(f"{sys.executable} -m pip install {package}", check=False):
            print(f"‚ö†Ô∏è  Error instalando {package}, continuando...")
            failed_packages.append(package)
        else:
            print(f"‚úÖ {package} instalado correctamente")
    
    # Intentar instalar bitsandbytes para Windows
    if platform.system() == "Windows":
        print(f"\nüì¶ Instalando bitsandbytes para Windows...")
        if not run_command(f"{sys.executable} -m pip install bitsandbytes-windows", check=False):
            print("‚ö†Ô∏è  bitsandbytes-windows fall√≥, continuando sin √©l...")
    
    if failed_packages:
        print(f"\n‚ö†Ô∏è  Paquetes que fallaron: {', '.join(failed_packages)}")
        print("   El sistema puede funcionar sin algunos de estos paquetes")
    
    return len(failed_packages) < len(core_packages) // 2  # Si fallan menos de la mitad, continuamos

def install_optional_packages():
    """Instala paquetes opcionales que pueden fallar"""
    print("\nüîß Instalando paquetes opcionales...")
    
    optional_packages = [
        ("auto-gptq==0.6.0", "Cuantizaci√≥n GPTQ (solo para GPU avanzadas)"),
        ("unstructured[pdf]", "Soporte extendido para PDF"),
        ("bitsandbytes", "Optimizaci√≥n de memoria (alternativo)")
    ]
    
    for package, description in optional_packages:
        print(f"\nüì¶ Intentando instalar {package} - {description}")
        if run_command(f"{sys.executable} -m pip install {package}", check=False):
            print(f"‚úÖ {package} instalado")
        else:
            print(f"‚ö†Ô∏è  {package} fall√≥ - no es cr√≠tico, continuando...")

def install_llama_cpp(gpu_support=False):
    """Instala llama-cpp-python con soporte GPU si es necesario"""
    print("\nü¶ô Instalando llama-cpp-python...")
    
    if gpu_support and platform.system() == "Windows":
        # Para Windows con GPU NVIDIA - versi√≥n m√°s compatible
        print("Instalando versi√≥n con soporte CUDA...")
        success = run_command(f"{sys.executable} -m pip install llama-cpp-python[cublas]", check=False)
        if not success:
            print("Intentando instalaci√≥n manual con CMAKE...")
            # Fallback manual
            os.environ["CMAKE_ARGS"] = "-DLLAMA_CUBLAS=on"
            os.environ["FORCE_CMAKE"] = "1"
            success = run_command(f"{sys.executable} -m pip install llama-cpp-python --no-cache-dir", check=False)
    elif platform.system() == "Darwin":  # macOS
        # Para Mac con Metal
        print("Instalando versi√≥n con soporte Metal...")
        os.environ["CMAKE_ARGS"] = "-DLLAMA_METAL=on"
        os.environ["FORCE_CMAKE"] = "1"
        success = run_command(f"{sys.executable} -m pip install llama-cpp-python --no-cache-dir", check=False)
    else:
        # CPU solamente
        print("Instalando versi√≥n CPU...")
        success = run_command(f"{sys.executable} -m pip install llama-cpp-python", check=False)
    
    if not success:
        print("‚ö†Ô∏è  llama-cpp-python fall√≥, intentando versi√≥n b√°sica...")
        success = run_command(f"{sys.executable} -m pip install llama-cpp-python", check=False)
    
    return success

def create_directories():
    """Crea los directorios necesarios"""
    print("\nüìÅ Creando directorios necesarios...")
    
    directories = [
        "SOURCE_DOCUMENTS",
        "DB", 
        "models",
        "local_chat_history"
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"‚úÖ Directorio creado: {directory}")

def create_windows_config():
    """Crea configuraci√≥n espec√≠fica para Windows"""
    print("\n‚öôÔ∏è  Creando configuraci√≥n para Windows...")
    
    # Archivo de configuraci√≥n que evita errores comunes
    config_content = """# Configuraci√≥n localGPT para Windows
import os
import sys

# Evitar errores de CUDA en Windows
os.environ.setdefault('CUDA_VERSION', '11.8')
os.environ.setdefault('CUDA_HOME', '')

# Configurar encoding para Windows
if sys.platform == 'win32':
    import locale
    locale.setlocale(locale.LC_ALL, 'C')
"""
    
    with open("windows_config.py", "w") as f:
        f.write(config_content)
    
    print("‚úÖ Configuraci√≥n de Windows creada")

def test_installation():
    """Prueba que los paquetes cr√≠ticos funcionen"""
    print("\nüß™ Probando instalaci√≥n...")
    
    critical_imports = [
        ("langchain", "LangChain"),
        ("transformers", "Transformers"),
        ("torch", "PyTorch"),
        ("chromadb", "ChromaDB"),
        ("streamlit", "Streamlit")
    ]
    
    failed_imports = []
    
    for module, name in critical_imports:
        try:
            __import__(module)
            print(f"‚úÖ {name} funciona correctamente")
        except ImportError as e:
            print(f"‚ùå {name} fall√≥: {e}")
            failed_imports.append(name)
    
    if failed_imports:
        print(f"\n‚ö†Ô∏è  M√≥dulos que fallaron: {', '.join(failed_imports)}")
        print("   Algunos features pueden no funcionar correctamente")
        return False
    else:
        print("\n‚úÖ Todas las importaciones cr√≠ticas funcionan!")
        return True

def download_sample_document():
    """Descarga documento de muestra para testing"""
    print("\nüìÑ Creando documento de muestra...")
    
    sample_file = "SOURCE_DOCUMENTS/manual_localgpt.txt"
    sample_content = """
MANUAL DE USO DE LOCALGPT

LocalGPT es una herramienta de inteligencia artificial que te permite hacer preguntas sobre tus documentos de forma completamente privada y local.

CARACTER√çSTICAS PRINCIPALES:
- Procesamiento 100% local en tu computadora
- Sin env√≠o de datos a servidores externos
- Soporte para m√∫ltiples formatos: PDF, TXT, DOCX, CSV, MD, HTML
- Interfaz de l√≠nea de comandos y web
- Memoria conversacional opcional

COMANDOS B√ÅSICOS:

1. PROCESAR DOCUMENTOS:
   python ingest.py
   - Analiza todos los documentos en SOURCE_DOCUMENTS/
   - Crea una base de datos vectorial local
   - Solo necesitas hacerlo una vez por cada conjunto nuevo de documentos

2. HACER PREGUNTAS:
   python run_localGPT.py
   - Inicia el modo conversacional
   - Escribe tus preguntas en lenguaje natural
   - Escribe 'exit' para salir

3. INTERFAZ WEB:
   python run_localGPT_API.py (en una terminal)
   python localGPTUI/localGPTUI.py (en otra terminal)
   Luego abre: http://localhost:5111/

OPCIONES AVANZADAS:
- --show_sources: Muestra las fuentes de las respuestas
- --use_history: Habilita memoria conversacional
- --save_qa: Guarda preguntas y respuestas
- --device_type cpu: Fuerza uso de CPU

FORMATOS SOPORTADOS:
- PDF: Documentos, libros, manuales
- TXT: Texto plano
- DOCX: Documentos de Word
- CSV: Hojas de c√°lculo
- MD: Archivos Markdown
- HTML: P√°ginas web

CONSEJOS DE USO:
- Organiza tus documentos por temas en subcarpetas
- Usa nombres descriptivos para tus archivos
- Para mejores resultados, usa documentos en ingl√©s
- Los documentos m√°s largos proporcionan m√°s contexto

RESOLUCI√ìN DE PROBLEMAS:
- Si hay errores de memoria, cierra otras aplicaciones
- Para problemas de GPU, usa --device_type cpu
- Si las respuestas son lentas, considera usar un modelo m√°s peque√±o

¬°Disfruta usando LocalGPT de forma privada y segura!
"""
    
    with open(sample_file, 'w', encoding='utf-8') as f:
        f.write(sample_content)
    print(f"‚úÖ Manual creado: {sample_file}")

def show_next_steps():
    """Muestra los siguientes pasos despu√©s de la instalaci√≥n"""
    print("\n" + "="*60)
    print("üéâ ¬°INSTALACI√ìN COMPLETADA!")
    print("="*60)
    print()
    print("üìã SIGUIENTES PASOS:")
    print()
    print("1. üìÑ A√±adir documentos:")
    print("   - Copia tus archivos a SOURCE_DOCUMENTS/")
    print("   - Formatos: PDF, TXT, DOCX, CSV, MD, HTML")
    print()
    print("2. üîÑ Procesar documentos:")
    print("   python ingest.py")
    print()
    print("3. üí¨ Hacer preguntas:")
    print("   python run_localGPT.py")
    print()
    print("4. üåê Interfaz web:")
    print("   Terminal 1: python run_localGPT_API.py")
    print("   Terminal 2: python localGPTUI/localGPTUI.py")
    print("   Navegador: http://localhost:5111/")
    print()
    print("üöÄ INICIO R√ÅPIDO:")
    print("   - Ejecuta: iniciar.bat")
    print("   - Selecciona opci√≥n del men√∫")
    print()
    print("‚ö†Ô∏è  PRIMERA EJECUCI√ìN:")
    print("   - Descargar√° modelos (~4-7GB)")
    print("   - Requiere conexi√≥n a internet inicialmente")
    print("   - Despu√©s funciona sin internet")
    print()
    print("üÜò SI HAY PROBLEMAS:")
    print("   - Revisa INSTALACION_ES.md")
    print("   - Usa: python run_localGPT.py --device_type cpu")
    print("   - GitHub: github.com/PromtEngineer/localGPT/issues")
    print()
    print("="*60)

def main():
    """Funci√≥n principal de instalaci√≥n mejorada"""
    print("üöÄ LocalGPT - Instalador Mejorado para Windows")
    print("="*55)
    
    # Verificar Python
    if not check_python_version():
        input("Presiona Enter para salir...")
        return 1
    
    # Detectar GPU
    has_gpu = check_gpu()
    
    # Crear configuraci√≥n para Windows
    create_windows_config()
    
    # Crear requirements seguros
    create_requirements_safe()
    
    # Instalar paquetes b√°sicos
    if not install_core_packages():
        print("\n‚ùå Error cr√≠tico en la instalaci√≥n")
        print("Revisa los errores arriba y verifica:")
        print("- Conexi√≥n a internet")
        print("- Espacio en disco")
        print("- Permisos de administrador")
        input("Presiona Enter para salir...")
        return 1
    
    # Instalar paquetes opcionales
    install_optional_packages()
    
    # Instalar llama-cpp-python
    if not install_llama_cpp(has_gpu):
        print("\n‚ö†Ô∏è  llama-cpp-python fall√≥")
        print("   Puedes intentar instalarlo manualmente m√°s tarde")
        print("   El sistema funcionar√° con CPU solamente")
    
    # Crear directorios
    create_directories()
    
    # Crear documento de muestra
    download_sample_document()
    
    # Probar instalaci√≥n
    test_installation()
    
    # Mostrar siguientes pasos
    show_next_steps()
    
    input("\nPresiona Enter para continuar...")
    return 0

if __name__ == "__main__":
    sys.exit(main())
