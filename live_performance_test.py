#!/usr/bin/env python3
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        LIVE PERFORMANCE TESTING                             â•‘
â•‘                        TESTING CON LOS MEJORES MODELOS                     â•‘
â•‘                                                                              â•‘
â•‘  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â•‘
â•‘  â–ˆ                                                                          â–ˆ  â•‘
â•‘  â–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—     â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆ  â•‘
â•‘  â–ˆ  â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆ  â•‘
â•‘  â–ˆ  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆ  â•‘
â•‘  â–ˆ  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•     â–ˆ  â•‘
â•‘  â–ˆ  â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆ  â•‘
â•‘  â–ˆ   â•šâ•â•â•â•â•â•â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•     â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•   â–ˆ  â•‘
â•‘  â–ˆ                                                                          â–ˆ  â•‘
â•‘  â–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â•‘
â•‘                                                                              â•‘
â•‘  [TESTING: Claude Opus 4.1, Gemini 2.5 Pro, GPT-5]                        â•‘
â•‘  [DOMAINS: Programming, Reasoning, Mathematics]                            â•‘
â•‘  [METRICS: Performance, Speed, Cost, Quality]                             â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import asyncio
import aiohttp
import time
import json
from typing import Dict, Any, List, Tuple
from dataclasses import dataclass
from enum import Enum

class TestDomain(Enum):
    """Dominios de testing"""
    PROGRAMMING = "programming"
    REASONING = "reasoning"
    MATHEMATICS = "mathematics"

@dataclass
class TestResult:
    """Resultado de test individual"""
    model: str
    domain: TestDomain
    query: str
    response: str
    score: float
    response_time: float
    cost: float
    tokens_used: int
    success: bool
    error: str = None

class LivePerformanceTester:
    """Sistema de testing en vivo con los mejores modelos"""
    
    def __init__(self):
        self.api_key = "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"
        self.url = "https://openrouter.ai/api/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://live-performance-test.local",
            "X-Title": "Live Performance Testing"
        }
        
        # ğŸ† LOS MEJORES MODELOS PREMIUM
        self.top_models = {
            "claude_opus_4_1": {
                "id": "anthropic/claude-opus-4.1",
                "name": "Claude Opus 4.1",
                "description": "Mejor modelo de cÃ³digo del mundo (74.5% SWE-bench)",
                "context": 200000,
                "cost_per_1k_input": 0.015,
                "cost_per_1k_output": 0.075
            },
            "gemini_2_5_pro": {
                "id": "google/gemini-2.5-pro",
                "name": "Gemini 2.5 Pro",
                "description": "Modelo multimodal con 1M tokens de contexto",
                "context": 1048576,
                "cost_per_1k_input": 0.00125,
                "cost_per_1k_output": 0.01
            },
            "gpt_5": {
                "id": "openai/gpt-5",
                "name": "GPT-5",
                "description": "Modelo mÃ¡s avanzado de OpenAI",
                "context": 400000,
                "cost_per_1k_input": 0.00000125,
                "cost_per_1k_output": 0.00001
            }
        }
        
        # ğŸ¯ QUERIES DE TESTING POR DOMINIO
        self.test_queries = {
            TestDomain.PROGRAMMING: [
                "Implementa un algoritmo de ordenamiento quicksort optimizado en Python con anÃ¡lisis de complejidad",
                "Crea una funciÃ³n que detecte si un grafo es bipartito usando BFS",
                "Desarrolla un sistema de cachÃ© LRU con complejidad O(1) para todas las operaciones"
            ],
            TestDomain.REASONING: [
                "Analiza la complejidad computacional del problema del viajante y propÃ³n una soluciÃ³n aproximada",
                "Explica paso a paso cÃ³mo resolver el problema de las 8 reinas usando backtracking",
                "Demuestra por quÃ© el algoritmo de Dijkstra no funciona con pesos negativos"
            ],
            TestDomain.MATHEMATICS: [
                "Demuestra la fÃ³rmula de Euler e^(iÏ€) + 1 = 0 usando series de Taylor",
                "Calcula la derivada de la funciÃ³n f(x) = ln(sin(x^2)) usando la regla de la cadena",
                "Resuelve la ecuaciÃ³n diferencial dy/dx + 2y = e^(-x) con condiciÃ³n inicial y(0) = 1"
            ]
        }
        
        self.results = []
        
    def print_header(self):
        """Imprime header del sistema de testing"""
        print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘                        LIVE PERFORMANCE TESTING                             â•‘")
        print("â•‘                        TESTING CON LOS MEJORES MODELOS                     â•‘")
        print("â•‘                                                                              â•‘")
        print("â•‘  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â•‘")
        print("â•‘  â–ˆ                                                                          â–ˆ  â•‘")
        print("â•‘  â–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—     â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆ  â•‘")
        print("â•‘  â–ˆ  â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆ  â•‘")
        print("â•‘  â–ˆ  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆ  â•‘")
        print("â•‘  â–ˆ  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•     â–ˆ  â•‘")
        print("â•‘  â–ˆ  â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆ  â•‘")
        print("â•‘  â–ˆ   â•šâ•â•â•â•â•â•â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•     â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•   â–ˆ  â•‘")
        print("â•‘  â–ˆ                                                                          â–ˆ  â•‘")
        print("â•‘  â–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â•‘")
        print("â•‘                                                                              â•‘")
        print("â•‘  [TESTING: Claude Opus 4.1, Gemini 2.5 Pro, GPT-5]                        â•‘")
        print("â•‘  [DOMAINS: Programming, Reasoning, Mathematics]                            â•‘")
        print("â•‘  [METRICS: Performance, Speed, Cost, Quality]                             â•‘")
        print("â•‘                                                                              â•‘")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    async def call_model(self, model_id: str, prompt: str) -> Dict[str, Any]:
        """Llamada al modelo especÃ­fico"""
        
        payload = {
            "model": model_id,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 4000,
            "temperature": 0.1
        }
        
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.url,
                    headers=self.headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=120)
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        content = data['choices'][0]['message']['content']
                        usage = data.get('usage', {})
                        
                        input_tokens = usage.get('prompt_tokens', 0)
                        output_tokens = usage.get('completion_tokens', 0)
                        
                        response_time = time.time() - start_time
                        
                        return {
                            "success": True,
                            "response": content,
                            "input_tokens": input_tokens,
                            "output_tokens": output_tokens,
                            "response_time": response_time
                        }
                    else:
                        error_text = await response.text()
                        return {
                            "success": False,
                            "error": f"HTTP {response.status}: {error_text}",
                            "response_time": time.time() - start_time
                        }
                        
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "response_time": time.time() - start_time
            }
    
    def calculate_score(self, response: str, domain: TestDomain) -> float:
        """Calcular score basado en la calidad de la respuesta"""
        
        if not response:
            return 0.0
        
        score = 0.0
        response_lower = response.lower()
        
        # MÃ©tricas base
        if "```" in response:
            score += 0.2
        if any(keyword in response_lower for keyword in ["def ", "class ", "function", "return"]):
            score += 0.15
        if any(word in response_lower for word in ["explic", "paso", "proceso", "mÃ©todo"]):
            score += 0.15
        if any(word in response_lower for word in ["algoritmo", "lÃ³gica", "estrategia"]):
            score += 0.1
        if any(word in response_lower for word in ["complejidad", "optimiz", "eficien"]):
            score += 0.1
        if len(response) > 500:
            score += 0.1
        
        # Ajustes especÃ­ficos por dominio
        if domain == TestDomain.PROGRAMMING:
            if any(char in response for char in ["def ", "class ", "import ", "return"]):
                score += 0.2
            if any(word in response_lower for word in ["algoritmo", "complejidad", "tiempo", "espacio"]):
                score += 0.15
        elif domain == TestDomain.REASONING:
            if any(word in response_lower for word in ["anÃ¡lisis", "lÃ³gico", "sistemÃ¡tico", "metodolÃ³gico"]):
                score += 0.2
            if any(word in response_lower for word in ["paso", "proceso", "mÃ©todo", "enfoque"]):
                score += 0.15
        elif domain == TestDomain.MATHEMATICS:
            if any(char in response for char in ["âˆ«", "âˆ‘", "Ï€", "âˆ", "âˆš", "=", "â‰ ", "â‰¤", "â‰¥"]):
                score += 0.2
            if any(word in response_lower for word in ["demostraciÃ³n", "teorema", "fÃ³rmula", "prueba", "matemÃ¡tica"]):
                score += 0.15
        
        return min(1.0, score)
    
    def calculate_cost(self, model_info: Dict, input_tokens: int, output_tokens: int) -> float:
        """Calcular costo de la llamada"""
        input_cost = (input_tokens / 1000) * model_info["cost_per_1k_input"]
        output_cost = (output_tokens / 1000) * model_info["cost_per_1k_output"]
        return input_cost + output_cost
    
    async def test_model_domain(self, model_key: str, domain: TestDomain) -> List[TestResult]:
        """Testear un modelo en un dominio especÃ­fico"""
        
        model_info = self.top_models[model_key]
        results = []
        
        print(f"â•‘  ğŸ§ª Testing {model_info['name']} en {domain.value.upper()}:")
        
        for i, query in enumerate(self.test_queries[domain], 1):
            print(f"â•‘     Query {i}: {query[:60]}...")
            
            # Llamada al modelo
            response_data = await self.call_model(model_info["id"], query)
            
            if response_data["success"]:
                # Calcular mÃ©tricas
                score = self.calculate_score(response_data["response"], domain)
                cost = self.calculate_cost(
                    model_info,
                    response_data["input_tokens"],
                    response_data["output_tokens"]
                )
                
                result = TestResult(
                    model=model_info["name"],
                    domain=domain,
                    query=query,
                    response=response_data["response"],
                    score=score,
                    response_time=response_data["response_time"],
                    cost=cost,
                    tokens_used=response_data["input_tokens"] + response_data["output_tokens"],
                    success=True
                )
                
                results.append(result)
                
                status_icon = "âœ…" if score > 0.7 else "âš ï¸" if score > 0.5 else "âŒ"
                print(f"â•‘       {status_icon} Score: {score:.3f} | Time: {response_data['response_time']:.2f}s | Cost: ${cost:.6f}")
            else:
                result = TestResult(
                    model=model_info["name"],
                    domain=domain,
                    query=query,
                    response="",
                    score=0.0,
                    response_time=response_data["response_time"],
                    cost=0.0,
                    tokens_used=0,
                    success=False,
                    error=response_data["error"]
                )
                
                results.append(result)
                print(f"â•‘       âŒ Error: {response_data['error']}")
        
        return results
    
    async def run_comprehensive_testing(self):
        """Ejecutar testing completo con todos los modelos"""
        
        print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘  LIVE PERFORMANCE TESTING - INICIANDO TESTING COMPLETO")
        print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
        print("â•‘  Modelos a testear:")
        for key, model in self.top_models.items():
            print(f"â•‘  â€¢ {model['name']}: {model['description']}")
        print("â•‘  Dominios: Programming, Reasoning, Mathematics")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        # Testing por modelo y dominio
        for model_key in self.top_models.keys():
            print(f"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
            print(f"â•‘  TESTING {self.top_models[model_key]['name'].upper()}")
            print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            
            for domain in TestDomain:
                domain_results = await self.test_model_domain(model_key, domain)
                self.results.extend(domain_results)
                
                # Pausa entre dominios
                await asyncio.sleep(2)
            
            # Pausa entre modelos
            await asyncio.sleep(5)
        
        # AnÃ¡lisis de resultados
        self.analyze_results()
    
    def analyze_results(self):
        """Analizar y mostrar resultados del testing"""
        
        print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘  ANÃLISIS DE RESULTADOS - LIVE PERFORMANCE TESTING")
        print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
        
        # Agrupar resultados por modelo
        model_results = {}
        for result in self.results:
            if result.model not in model_results:
                model_results[result.model] = []
            model_results[result.model].append(result)
        
        # AnÃ¡lisis por modelo
        for model_name, results in model_results.items():
            print(f"â•‘  ğŸ“Š {model_name}:")
            
            # MÃ©tricas generales
            successful_results = [r for r in results if r.success]
            if successful_results:
                avg_score = sum(r.score for r in successful_results) / len(successful_results)
                avg_time = sum(r.response_time for r in successful_results) / len(successful_results)
                total_cost = sum(r.cost for r in successful_results)
                total_tokens = sum(r.tokens_used for r in successful_results)
                
                print(f"â•‘     â€¢ Score Promedio: {avg_score:.3f}")
                print(f"â•‘     â€¢ Tiempo Promedio: {avg_time:.2f}s")
                print(f"â•‘     â€¢ Costo Total: ${total_cost:.6f}")
                print(f"â•‘     â€¢ Tokens Usados: {total_tokens:,}")
                print(f"â•‘     â€¢ Tasa de Ã‰xito: {len(successful_results)}/{len(results)} ({len(successful_results)/len(results)*100:.1f}%)")
            else:
                print(f"â•‘     â€¢ âŒ Sin resultados exitosos")
            
            # AnÃ¡lisis por dominio
            for domain in TestDomain:
                domain_results = [r for r in results if r.domain == domain and r.success]
                if domain_results:
                    domain_avg_score = sum(r.score for r in domain_results) / len(domain_results)
                    print(f"â•‘     â€¢ {domain.value.title()}: {domain_avg_score:.3f}")
        
        # ComparaciÃ³n entre modelos
        print("\nâ•‘  ğŸ† COMPARACIÃ“N ENTRE MODELOS:")
        print("â•‘  " + "="*60)
        
        comparison_data = []
        for model_name, results in model_results.items():
            successful_results = [r for r in results if r.success]
            if successful_results:
                avg_score = sum(r.score for r in successful_results) / len(successful_results)
                avg_time = sum(r.response_time for r in successful_results) / len(successful_results)
                total_cost = sum(r.cost for r in successful_results)
                
                comparison_data.append({
                    "model": model_name,
                    "avg_score": avg_score,
                    "avg_time": avg_time,
                    "total_cost": total_cost
                })
        
        # Ordenar por score
        comparison_data.sort(key=lambda x: x["avg_score"], reverse=True)
        
        for i, data in enumerate(comparison_data, 1):
            medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰"
            print(f"â•‘  {medal} {data['model']}: Score {data['avg_score']:.3f} | Time {data['avg_time']:.2f}s | Cost ${data['total_cost']:.6f}")
        
        # Guardar resultados
        self.save_results()
    
    def save_results(self):
        """Guardar resultados en archivo"""
        
        results_data = {
            "timestamp": time.time(),
            "models_tested": list(self.top_models.keys()),
            "results": [
                {
                    "model": r.model,
                    "domain": r.domain.value,
                    "query": r.query,
                    "response": r.response,
                    "score": r.score,
                    "response_time": r.response_time,
                    "cost": r.cost,
                    "tokens_used": r.tokens_used,
                    "success": r.success,
                    "error": r.error
                }
                for r in self.results
            ]
        }
        
        filename = f"live_performance_results_{int(time.time())}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nâ•‘  ğŸ’¾ Resultados guardados en: {filename}")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

async def main():
    """FunciÃ³n principal de testing en vivo"""
    
    tester = LivePerformanceTester()
    tester.print_header()
    
    await tester.run_comprehensive_testing()

if __name__ == "__main__":
    asyncio.run(main())
